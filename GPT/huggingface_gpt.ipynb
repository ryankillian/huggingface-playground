{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8141cf8bb2e34e3198354abcf1210e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a65c6020a6e64bb5beec396af5c4a7f1",
              "IPY_MODEL_bac44a0ada234a09b6754e2ff24e6c9c",
              "IPY_MODEL_1c1c2d7c0a804420907591e45c016464"
            ],
            "layout": "IPY_MODEL_a3b061505e3046f5aadee85d6fec6e39"
          }
        },
        "a65c6020a6e64bb5beec396af5c4a7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39f2bc400c90404a8a65cd34c310ece7",
            "placeholder": "​",
            "style": "IPY_MODEL_e9974d3982444aaab4190365c9bd8195",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "bac44a0ada234a09b6754e2ff24e6c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba11bc8db5f34ca78855422a251db1b1",
            "max": 74,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49692744d03a473dbb48864e9fdd8064",
            "value": 74
          }
        },
        "1c1c2d7c0a804420907591e45c016464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31895ac76299437ebe3ab19246371552",
            "placeholder": "​",
            "style": "IPY_MODEL_aca72e2fcbc4496ab4a3da664512449b",
            "value": " 74.0/74.0 [00:00&lt;00:00, 1.25kB/s]"
          }
        },
        "a3b061505e3046f5aadee85d6fec6e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f2bc400c90404a8a65cd34c310ece7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9974d3982444aaab4190365c9bd8195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba11bc8db5f34ca78855422a251db1b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49692744d03a473dbb48864e9fdd8064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31895ac76299437ebe3ab19246371552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca72e2fcbc4496ab4a3da664512449b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17cd912f0c77478c882f6fddbb191a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb6199a801e64673b657fa03a8d725d1",
              "IPY_MODEL_e4b8a5e8d7484be597636f5edf24d4a5",
              "IPY_MODEL_37609ed806604091bb3d978ef310a352"
            ],
            "layout": "IPY_MODEL_ad74ef86c77a4c96965b988ba5d8d05c"
          }
        },
        "fb6199a801e64673b657fa03a8d725d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3271584b343c4ebba2db6fc9d07363ee",
            "placeholder": "​",
            "style": "IPY_MODEL_cb0e472080654a0ca17f42ec2d68be60",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "e4b8a5e8d7484be597636f5edf24d4a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdb448406f4d4feabbda0f0ca2716143",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d24932b526ea41a491a5911d5af45b41",
            "value": 665
          }
        },
        "37609ed806604091bb3d978ef310a352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f5d14a5b8ae4e47ad05bba8d8b3524d",
            "placeholder": "​",
            "style": "IPY_MODEL_3ad4816662804119aa1594cdefabc89a",
            "value": " 665/665 [00:00&lt;00:00, 21.5kB/s]"
          }
        },
        "ad74ef86c77a4c96965b988ba5d8d05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3271584b343c4ebba2db6fc9d07363ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0e472080654a0ca17f42ec2d68be60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdb448406f4d4feabbda0f0ca2716143": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24932b526ea41a491a5911d5af45b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f5d14a5b8ae4e47ad05bba8d8b3524d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad4816662804119aa1594cdefabc89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f45b05dec514df6b935e3fa50bf24b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bb565ccd3b54184bb673a68c1670a07",
              "IPY_MODEL_d7b1053804da4c2aa759c8b75f428728",
              "IPY_MODEL_6874d8cae2ba4edd8f4e6dcc7a04d47c"
            ],
            "layout": "IPY_MODEL_9b1f7c54878149b5ae5d2e7cdac97719"
          }
        },
        "3bb565ccd3b54184bb673a68c1670a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9cec6aad0dd40d5b1c689894865bfa7",
            "placeholder": "​",
            "style": "IPY_MODEL_6d277d7ed4cd47c096ce814f6c25d0ae",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "d7b1053804da4c2aa759c8b75f428728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c8fb586a82449e79356b2aecc605777",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bf25767fb25445bb17924968905e8fe",
            "value": 548105171
          }
        },
        "6874d8cae2ba4edd8f4e6dcc7a04d47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f08f026b8024cdc8ebaedc3ec332c69",
            "placeholder": "​",
            "style": "IPY_MODEL_3d6b05c39dad42a58910855fbf0f8559",
            "value": " 548M/548M [00:02&lt;00:00, 204MB/s]"
          }
        },
        "9b1f7c54878149b5ae5d2e7cdac97719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9cec6aad0dd40d5b1c689894865bfa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d277d7ed4cd47c096ce814f6c25d0ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c8fb586a82449e79356b2aecc605777": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf25767fb25445bb17924968905e8fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f08f026b8024cdc8ebaedc3ec332c69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6b05c39dad42a58910855fbf0f8559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b78de74dacc4defbb0bfde5cc1c8697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c5742a75ae94d7db2f22d0bbe042862",
              "IPY_MODEL_8604830df60d4195b58c38a2c715b5bb",
              "IPY_MODEL_7bf1da3efb6a4fb8bfefafca0f3f7b50"
            ],
            "layout": "IPY_MODEL_dda94923e23e430aada03b207d8fadda"
          }
        },
        "4c5742a75ae94d7db2f22d0bbe042862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a8d02474bc84748abd6dd69857bc99b",
            "placeholder": "​",
            "style": "IPY_MODEL_af46d674e1a749dcbee31ef3f90071c4",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "8604830df60d4195b58c38a2c715b5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_668b42aad898461c9b57bba3124557bd",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17425ab6804045dfa1e58acfb001cf81",
            "value": 124
          }
        },
        "7bf1da3efb6a4fb8bfefafca0f3f7b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23835132e2544828b027ffc71e9062e1",
            "placeholder": "​",
            "style": "IPY_MODEL_6f31ffcae0554f1caa6fdded9566f97f",
            "value": " 124/124 [00:00&lt;00:00, 5.44kB/s]"
          }
        },
        "dda94923e23e430aada03b207d8fadda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a8d02474bc84748abd6dd69857bc99b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af46d674e1a749dcbee31ef3f90071c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "668b42aad898461c9b57bba3124557bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17425ab6804045dfa1e58acfb001cf81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23835132e2544828b027ffc71e9062e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f31ffcae0554f1caa6fdded9566f97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90769f4a744247d3ae7355c34a0ea499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c54124f622045089adcb74115b46d2b",
              "IPY_MODEL_78c25d5295dd4a08a5b5bb194a9718fd",
              "IPY_MODEL_08a35c4c4fe74e93a608298f8194d7ff"
            ],
            "layout": "IPY_MODEL_e104a4f60ef440cf839c92563ce0d821"
          }
        },
        "9c54124f622045089adcb74115b46d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45e046dfb2bc453eb484ee6a3f35f8b7",
            "placeholder": "​",
            "style": "IPY_MODEL_bdd1880d9f8342aaacaa8605844c7c84",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "78c25d5295dd4a08a5b5bb194a9718fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2d06be84a36445c88db1c44b576d703",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d86741e7f6d045f284fcade6272ec21b",
            "value": 1042301
          }
        },
        "08a35c4c4fe74e93a608298f8194d7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6fa9a2329704cc9818aa87612edf9c4",
            "placeholder": "​",
            "style": "IPY_MODEL_bff37a13fa0c4ed5af904d07e48aa733",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 5.09MB/s]"
          }
        },
        "e104a4f60ef440cf839c92563ce0d821": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45e046dfb2bc453eb484ee6a3f35f8b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd1880d9f8342aaacaa8605844c7c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2d06be84a36445c88db1c44b576d703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d86741e7f6d045f284fcade6272ec21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6fa9a2329704cc9818aa87612edf9c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bff37a13fa0c4ed5af904d07e48aa733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5869141077f4093bcd29946958a9e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_146ac458a5c74c7ba5b29cf4e9035671",
              "IPY_MODEL_74617a48fafc4fef96962ccf3641c47e",
              "IPY_MODEL_5d5b079711f94981be863fbfb2874a45"
            ],
            "layout": "IPY_MODEL_e56ab3f8f0c44c35baab508f169f5782"
          }
        },
        "146ac458a5c74c7ba5b29cf4e9035671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e0591866a6941229fc2eb49b12336a8",
            "placeholder": "​",
            "style": "IPY_MODEL_6b749a8afb834559a1ba414ee07dcdfe",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "74617a48fafc4fef96962ccf3641c47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_990e88d1778d47258567c77095c7c4ef",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_529a2cc1ff3b41b1afd37b73da1b80f2",
            "value": 456318
          }
        },
        "5d5b079711f94981be863fbfb2874a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83175d25607443ce88cfcf502770d54f",
            "placeholder": "​",
            "style": "IPY_MODEL_b033bd878deb4f60b5d10a19aecc8e96",
            "value": " 456k/456k [00:00&lt;00:00, 3.51MB/s]"
          }
        },
        "e56ab3f8f0c44c35baab508f169f5782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e0591866a6941229fc2eb49b12336a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b749a8afb834559a1ba414ee07dcdfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "990e88d1778d47258567c77095c7c4ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "529a2cc1ff3b41b1afd37b73da1b80f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83175d25607443ce88cfcf502770d54f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b033bd878deb4f60b5d10a19aecc8e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fc905d677c24bc1a49ede52311baec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62f05cbd67844bd180c4929281e4e1f0",
              "IPY_MODEL_8646a0f0da4b48bcbf15b780cc808c2d",
              "IPY_MODEL_5177460348d9423a85005a46f0a0f4e2"
            ],
            "layout": "IPY_MODEL_e630be3f5fea41e99dd2a4dced9ffbcb"
          }
        },
        "62f05cbd67844bd180c4929281e4e1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_017497ee1fe84f0eb1537d8dcc74c2be",
            "placeholder": "​",
            "style": "IPY_MODEL_169c48465a694732b48d13dedbe4157c",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "8646a0f0da4b48bcbf15b780cc808c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3a2dcf6c9f74a1ca187ffabeef2442a",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8399f77de4b43f195b6f8e6d7d13ed2",
            "value": 718
          }
        },
        "5177460348d9423a85005a46f0a0f4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e317422e60454c8ca4ca50449ba3d820",
            "placeholder": "​",
            "style": "IPY_MODEL_1bc010879c3d4420be0a0bdcd5ba4814",
            "value": " 718/718 [00:00&lt;00:00, 27.0kB/s]"
          }
        },
        "e630be3f5fea41e99dd2a4dced9ffbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "017497ee1fe84f0eb1537d8dcc74c2be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "169c48465a694732b48d13dedbe4157c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3a2dcf6c9f74a1ca187ffabeef2442a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8399f77de4b43f195b6f8e6d7d13ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e317422e60454c8ca4ca50449ba3d820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bc010879c3d4420be0a0bdcd5ba4814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68fc69c2bd4e4ee1ba7c1f2c06c14078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b004f466e649451db9ef66724eccec4f",
              "IPY_MODEL_acd2ab3d7b37499a9b39ead7a1a93cba",
              "IPY_MODEL_b41fae4b6f824e718d94dfc130475071"
            ],
            "layout": "IPY_MODEL_16a524e354534c3db34a654c1c82549e"
          }
        },
        "b004f466e649451db9ef66724eccec4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fd5b8c0ce5b4ef48a60e29a28dbe2dc",
            "placeholder": "​",
            "style": "IPY_MODEL_46e894c3fee746c4a3b7505c8676d259",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "acd2ab3d7b37499a9b39ead7a1a93cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a916b0cc4c2d4663b2f4982562afe3bd",
            "max": 1519984962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d4966ee9bc3449186ef5e18029e331c",
            "value": 1519984962
          }
        },
        "b41fae4b6f824e718d94dfc130475071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66e5962e7a924019a096cc8dde94df22",
            "placeholder": "​",
            "style": "IPY_MODEL_9572e154a90b42ebb9cf40ec36129745",
            "value": " 1.52G/1.52G [00:11&lt;00:00, 216MB/s]"
          }
        },
        "16a524e354534c3db34a654c1c82549e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fd5b8c0ce5b4ef48a60e29a28dbe2dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e894c3fee746c4a3b7505c8676d259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a916b0cc4c2d4663b2f4982562afe3bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d4966ee9bc3449186ef5e18029e331c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66e5962e7a924019a096cc8dde94df22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9572e154a90b42ebb9cf40ec36129745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0759554d6cd4eaba770528538304357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_406ec85504d84edd911436f20c53a74e",
              "IPY_MODEL_962ea5e09ac441e89f8352da48f2c1c9",
              "IPY_MODEL_2c0eb383301b4f8282fa3368dbf1242b"
            ],
            "layout": "IPY_MODEL_f2b8b83cc0db47489c587fc53f0d1bc0"
          }
        },
        "406ec85504d84edd911436f20c53a74e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03082ac336a94b90a5aa8c72da56e214",
            "placeholder": "​",
            "style": "IPY_MODEL_d91d9d64eecb46e8931e76e86b5ff83b",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "962ea5e09ac441e89f8352da48f2c1c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf14519e3b0d4debac19a092a4bc564b",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30563ffa6a9d423d9dd2ca6e96b5ddbc",
            "value": 124
          }
        },
        "2c0eb383301b4f8282fa3368dbf1242b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd68abdd6588411784d9d72c00dc2cd5",
            "placeholder": "​",
            "style": "IPY_MODEL_e60b8bbd87674ccfa360262f26778d70",
            "value": " 124/124 [00:00&lt;00:00, 8.04kB/s]"
          }
        },
        "f2b8b83cc0db47489c587fc53f0d1bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03082ac336a94b90a5aa8c72da56e214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d91d9d64eecb46e8931e76e86b5ff83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf14519e3b0d4debac19a092a4bc564b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30563ffa6a9d423d9dd2ca6e96b5ddbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd68abdd6588411784d9d72c00dc2cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e60b8bbd87674ccfa360262f26778d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09f7311eb5e24543a80dbe365fc72c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ded964ff9264d439d3f612045a2a27b",
              "IPY_MODEL_d65ee7c4b8b34c26afa657a5d3f86291",
              "IPY_MODEL_d901ba3e18304885a2df3a4d83257d5f"
            ],
            "layout": "IPY_MODEL_eacb1eaf4cca4c5d8987d6b84517de0f"
          }
        },
        "3ded964ff9264d439d3f612045a2a27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30b6cce3f9224b79bda52a32e0f899f7",
            "placeholder": "​",
            "style": "IPY_MODEL_8451e7baff73406a988b1a60958c824e",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "d65ee7c4b8b34c26afa657a5d3f86291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7daa2ac6ea70467b876059de116d400e",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a52e13458d664254997bb61bba3e0160",
            "value": 1042301
          }
        },
        "d901ba3e18304885a2df3a4d83257d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_780b3b3016cd4694959829d6eca86679",
            "placeholder": "​",
            "style": "IPY_MODEL_98e9ee3fd0b747bb83bdb4341edc1c46",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 5.25MB/s]"
          }
        },
        "eacb1eaf4cca4c5d8987d6b84517de0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30b6cce3f9224b79bda52a32e0f899f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8451e7baff73406a988b1a60958c824e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7daa2ac6ea70467b876059de116d400e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a52e13458d664254997bb61bba3e0160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "780b3b3016cd4694959829d6eca86679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e9ee3fd0b747bb83bdb4341edc1c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2682a9ca8d04ed3bf878c0b0711efef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2855331553f40559b07858f9c45fec5",
              "IPY_MODEL_93088a4ed7b24d2a88e02a059144684b",
              "IPY_MODEL_8a8cbeb270dd4805acf512441e22c4e2"
            ],
            "layout": "IPY_MODEL_fb1bf5f7b959472aa72c6ab25328f7b8"
          }
        },
        "e2855331553f40559b07858f9c45fec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f5338c1653940b28085cce29aad844e",
            "placeholder": "​",
            "style": "IPY_MODEL_9cabbfd2b23c4098be5620d354fb58fd",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "93088a4ed7b24d2a88e02a059144684b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c78b9fd4945c4b699f9977a0483860a7",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64762c9b3bf74da8b4dedb62561e34af",
            "value": 456318
          }
        },
        "8a8cbeb270dd4805acf512441e22c4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58c2592333ec4e67acddffae2d1b7964",
            "placeholder": "​",
            "style": "IPY_MODEL_1e1569aac240478296e07b03ea8aeb16",
            "value": " 456k/456k [00:00&lt;00:00, 3.50MB/s]"
          }
        },
        "fb1bf5f7b959472aa72c6ab25328f7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5338c1653940b28085cce29aad844e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cabbfd2b23c4098be5620d354fb58fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c78b9fd4945c4b699f9977a0483860a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64762c9b3bf74da8b4dedb62561e34af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58c2592333ec4e67acddffae2d1b7964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e1569aac240478296e07b03ea8aeb16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Introduction to GPT-1 and GPT-2 with HuggingFace\n",
        "\n",
        "Welcome to this Colab notebook where we will explore and experiment with two groundbreaking models in the world of Natural Language Processing (NLP) - GPT-1 and GPT-2. These models, developed by OpenAI, have been at the forefront of advancing the capabilities of language models.\n",
        "\n",
        "## Overview:\n",
        "\n",
        "### GPT-1:\n",
        "The first iteration, GPT-1, laid the foundation for transformer-based architectures in NLP tasks. It introduced the concept of using a transformer structure without the need for task-specific training data, setting the stage for future models.\n",
        "\n",
        "#### Key Takeaways from our GPT-1 Exploration:\n",
        "1. **Model Variants**: GPT-1 offers several variants including the base model (`OpenAIGPTModel`) and versions with language modeling heads (`OpenAIGPTLMHeadModel`).\n",
        "2. **Embeddings**: We looked at how GPT-1 can be used to generate rich embeddings for downstream tasks.\n",
        "3. **Fine-tuning**: The possibility of fine-tuning GPT-1 for specific tasks was explored, showcasing its versatility.\n",
        "4. **Text Generation**: Using the `OpenAIGPTLMHeadModel`, we generated text based on given prompts.\n",
        "\n",
        "### GPT-2:\n",
        "Building on the success of GPT-1, GPT-2 brought improvements in size, training data, and architecture. It's known for its ability to generate coherent and contextually relevant paragraphs of text.\n",
        "\n",
        "#### Key Takeaways from our GPT-2 Exploration:\n",
        "1. **Model Sizes**: GPT-2 is available in different sizes, with the small, medium, large, and extra-large variants.\n",
        "2. **Improved Text Generation**: We delved into text generation capabilities of GPT-2, observing its prowess in generating more coherent and diverse text compared to its predecessor.\n",
        "3. **Chatbot Creation**: A step-by-step demonstration was provided to harness GPT-2's capabilities to create a basic chatbot.\n",
        "4. **Tokenization**: The importance of tokenization and attention mechanisms were highlighted, especially in the context of generating meaningful outputs.\n",
        "\n",
        "## Summary:\n",
        "Throughout this notebook, we used models available from the HuggingFace library, a popular platform for state-of-the-art NLP models. Our journey began with understanding the architecture and capabilities of GPT-1, moving on to its successor, GPT-2. Practical implementations, ranging from text generation to chatbot creation, provided hands-on experience and insights into the capabilities and potential applications of these models.\n",
        "\n",
        "Feel free to experiment, tweak parameters, and explore further to gain a deeper understanding of the world of Generative Pre-trained Transformers!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cSPOrM4pPrYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The original GPT\n",
        "\n",
        "The original GPT, developed by OpenAI, was a landmark in the application of Transformer architectures for language modeling. It paved the way for many successor models, most notably GPT-2 and GPT-3. When we discuss different \"models\" for GPT, we're primarily referring to variations in its architecture, size, and pre-trained weights. Let's dive into the different aspects step-by-step:\n",
        "\n",
        "### Step 1: Understanding the Architecture\n",
        "\n",
        "1. **Base Architecture**: The original GPT uses a Transformer architecture, specifically the decoder part of it. Unlike the full Transformer which has both an encoder and a decoder, GPT uses only the decoder stacks.\n",
        "\n",
        "2. **Direction of Information Flow**: The GPT model is trained to predict the next word in a sequence, so it's a unidirectional model. This means that each word can only attend to previous words in the sequence, not future words.\n",
        "\n",
        "### Step 2: Model Sizes\n",
        "\n",
        "The number of parameters in a model, which often correlates with its size and capacity, determines its ability to understand and generate intricate text patterns. For GPT:\n",
        "\n",
        "1. **Small**: This is the smallest variant with the fewest parameters. It's faster and requires less memory but might not perform as well as the larger variants on complex tasks.\n",
        "  \n",
        "2. **Medium**: A middle-ground in terms of size and performance.\n",
        "\n",
        "3. **Large**: The largest variant of the original GPT with the most parameters. It's expected to perform best on tasks but is slower and demands more memory.\n",
        "\n",
        "### Step 3: Pre-trained Weights\n",
        "\n",
        "1. **Pre-training on Large Corpora**: Like other Transformer models, GPT is pre-trained on large text corpora. This gives it a vast knowledge of language, enabling it to generate coherent and contextually relevant text.\n",
        "\n",
        "2. **Fine-tuning**: After pre-training, GPT can be fine-tuned on specific tasks, like translation, summarization, or question-answering, to make it more specialized.\n",
        "\n",
        "### Step 4: Model Variants in the Hugging Face Library\n",
        "\n",
        "If you're using the Hugging Face `transformers` library, you'll notice different classes for GPT:\n",
        "\n",
        "1. **OpenAIGPTModel**: This is the base model which outputs raw hidden states. It doesn't have a head attached, meaning you can't directly get the next word probabilities.\n",
        "\n",
        "2. **OpenAIGPTLMHeadModel**: This is the model with a language modeling head on top. It's used for tasks like text generation.\n",
        "\n",
        "3. **OpenAIGPTDoubleHeadsModel**: This model has two heads: one for language modeling and another for multiple choice tasks. It can be used when you have tasks that require both predicting the next word and selecting an answer from multiple choices.\n",
        "\n",
        "By understanding these aspects of the original GPT, you can make informed decisions about which variant to use based on your specific requirements, be it general language modeling, text generation, or more specialized tasks."
      ],
      "metadata": {
        "id": "GbKubLGwXpwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `OpenAIGPTModel`"
      ],
      "metadata": {
        "id": "1TTid1QgbbQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to explore `OpenAIGPTModel`\n",
        "\n",
        "The `OpenAIGPTModel` is the base model class which provides the raw hidden states of the GPT architecture. This means it doesn't have any specific head attached to it for tasks like language modeling or multiple-choice. Instead, it provides the foundational Transformer outputs which can be used for various downstream tasks.\n",
        "\n",
        "Let's explore the `OpenAIGPTModel` step-by-step:\n",
        "\n",
        "### Step 1: Install and Import Necessary Libraries\n",
        "\n",
        "```python\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import OpenAIGPTModel, OpenAIGPTTokenizer\n",
        "```\n",
        "\n",
        "### Step 2: Load the Tokenizer and Model\n",
        "\n",
        "```python\n",
        "model_name = 'openai-gpt'\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "model = OpenAIGPTModel.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "### Step 3: Tokenize an Input and Get Model Output\n",
        "\n",
        "```python\n",
        "input_text = \"Hello, my dog is cute\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "```\n",
        "\n",
        "The `last_hidden_states` here contains the raw hidden activations from the final Transformer layer for each token in your input.\n",
        "\n",
        "### Step 4: Inspect the Output\n",
        "\n",
        "You can examine the shape and values of the `last_hidden_states`:\n",
        "\n",
        "```python\n",
        "print(last_hidden_states.shape)\n",
        "```\n",
        "\n",
        "This will give you a shape likely in the format `[1, num_tokens, hidden_size]`, where `num_tokens` is the token count in your input and `hidden_size` is the size of the hidden layer (typically 768 for the base model).\n",
        "\n",
        "### Step 5: Interpretation\n",
        "\n",
        "- The output you get from this model are essentially embeddings for each token in your input sentence. These embeddings are context-aware, meaning they represent the meaning of each word in the context of the given sentence.\n",
        "  \n",
        "- Since there's no specific head (like a language modeling or classification head) attached to this model, the embeddings aren't directly interpretable as probabilities or labels. Instead, they serve as rich, high-dimensional representations of your input text.\n",
        "\n",
        "- You can use these embeddings as input features for various downstream tasks, such as classification, clustering, or even further fine-tuning with a custom head.\n",
        "\n",
        "Remember, the main distinction of the `OpenAIGPTModel` is that it provides raw, context-rich embeddings without any task-specific predictions. It offers the foundational outputs of the GPT architecture. If you want outputs tailored to specific tasks (like text generation or multiple-choice selection), you'd use one of the other GPT model classes."
      ],
      "metadata": {
        "id": "lSwgXw-BYE7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `OpenAIGPTModel`: From Input to Output\n",
        "\n",
        "\n",
        "### Processing - From Input to Output\n",
        "\n",
        "1. **Tokenization**:\n",
        "    - The input text is tokenized into subwords or words. This is done using a predefined vocabulary that the tokenizer has been trained on.\n",
        "    - The tokens are then converted into integer IDs that correspond to embeddings in the model's embedding layer.\n",
        "\n",
        "2. **Embedding Layer**:\n",
        "    - The token IDs are passed through an embedding layer that converts each token ID into a high-dimensional vector. This vector is a learned representation of each token.\n",
        "\n",
        "3. **Transformer Layers**:\n",
        "    - These embeddings are then processed by several Transformer decoder blocks. Each block has two main components:\n",
        "        - **Self Attention Mechanism**: This allows the model to weigh the importance of different words in the input relative to a particular word. It's how context is incorporated into each token's representation.\n",
        "        - **Feed-Forward Neural Network**: After the attention scores are computed and applied, the result for each token is passed through a feed-forward network.\n",
        "    - As data passes through each successive Transformer layer, the representations become more abstract and context-rich.\n",
        "\n",
        "4. **Output**:\n",
        "    - The output of the final Transformer layer is what's returned by `OpenAIGPTModel`. These are the \"last hidden states\" and represent context-aware embeddings for each token in the input.\n",
        "\n"
      ],
      "metadata": {
        "id": "qcii5l_1Z_l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture in Terms of Tensor Sizes\n",
        "\n",
        "### Step 1: Embedding Size\n",
        "\n",
        "Yes, for the base version of the GPT model, each token in the input is represented by a 768-dimensional vector after being passed through the model's embedding layer.\n",
        "\n",
        "**Terminology**:\n",
        "- The 768-dimensional vector is often referred to as an \"embedding\" or a \"representation\". The number 768 is the \"hidden size\" or \"embedding size\" of the model.\n",
        "\n",
        "### Step 2: Model Code on GitHub\n",
        "\n",
        "The Hugging Face `transformers` library, which provides implementations of GPT (and many other models), is open-source and available on GitHub. You can find the code for GPT and its variants in this repository: [Hugging Face Transformers GitHub](https://github.com/huggingface/transformers)\n",
        "\n",
        "Inside the repository, the model-specific code for GPT can be found under the `modeling_openai.py` file (or a similarly named file, as the structure might evolve).\n",
        "\n",
        "### Step 3: Describing the Architecture in Terms of Tensor Sizes\n",
        "\n",
        "Let's take an input sentence of `N` tokens. Here's how the tensor sizes change through the GPT model:\n",
        "\n",
        "1. **Input Tokens**:\n",
        "    - Shape: \\([1, N]\\)\n",
        "    - The input is a sequence of token IDs.\n",
        "\n",
        "2. **Embedding Layer**:\n",
        "    - Shape: \\([1, N, 768]\\)\n",
        "    - Each token ID is mapped to a 768-dimensional embedding.\n",
        "\n",
        "3. **Transformer Layers**:\n",
        "    - GPT has multiple such layers. For each layer:\n",
        "        - **Self Attention**:\n",
        "            - **Query, Key, Value matrices**: Shape \\([1, N, 768]\\)\n",
        "            - **Attention Scores**: Shape \\([1, N, N]\\)\n",
        "            - **Attention Output**: Shape \\([1, N, 768]\\)\n",
        "        - **Feed-Forward Neural Network**:\n",
        "            - Maintains the shape \\([1, N, 768]\\) throughout.\n",
        "\n",
        "    The number of Transformer layers depends on the specific variant of GPT you're using. The base model has 12 layers, meaning the above operations are repeated 12 times.\n",
        "\n",
        "4. **Output**:\n",
        "    - Shape: \\([1, N, 768]\\)\n",
        "    - After the final Transformer layer, you get the context-rich representations for each of the `N` tokens.\n",
        "\n",
        "This is a high-level overview. Inside each Transformer layer, there are more operations like layer normalization and residual connections, but the tensor sizes mentioned above remain consistent through those operations.\n",
        "\n",
        "By understanding the tensor sizes at each step, you get a clearer picture of how data flows through the model and how the final representations are derived."
      ],
      "metadata": {
        "id": "Y3pWtITcdwej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `OpenAIGPTTokenizer`\n",
        "\n",
        "The tokenizer plays a crucial role in preparing the input data for the model. Let's dive into the details of the tokenizer used for the original GPT (`OpenAIGPTTokenizer`):\n",
        "\n",
        "### Step 1: Role of a Tokenizer\n",
        "\n",
        "A tokenizer's primary role is to convert input text into a format suitable for the model. This typically involves:\n",
        "1. Splitting the text into smaller chunks called tokens.\n",
        "2. Mapping each token to a unique integer ID, which the model can process.\n",
        "\n",
        "### Step 2: Type of Tokenizer for GPT\n",
        "\n",
        "For GPT, the tokenizer used is a Byte-Pair Encoding (BPE) tokenizer.\n",
        "\n",
        "### Step 3: Understanding BPE\n",
        "\n",
        "Byte-Pair Encoding (BPE) is a subword tokenizer. It starts with character-level tokenization and then iteratively merges frequent pairs of characters or character sequences to form more complex tokens. This continues until a predefined number of merges have been made, which results in a vocabulary of both full words and common subwords or character combinations.\n",
        "\n",
        "**Advantages**:\n",
        "1. **Flexibility**: It can handle words not seen during training because it can break them down into known subword units.\n",
        "2. **Efficiency**: BPE strikes a balance between character-level tokenization (which is very granular and results in long sequences) and word-level tokenization (which can't handle out-of-vocabulary words).\n",
        "\n",
        "### Step 4: Special Tokens\n",
        "\n",
        "The tokenizer also handles special tokens, such as:\n",
        "- `<|endoftext|>`: Used to indicate the end of a text in GPT.\n",
        "- Others like padding tokens, if defined.\n",
        "\n",
        "### Step 5: Methods in the Tokenizer\n",
        "\n",
        "When you call methods like `tokenizer.encode()` or `tokenizer.decode()`, you're utilizing the BPE process and the model's specific vocabulary to convert between text and token IDs.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The `OpenAIGPTTokenizer` is a BPE-based tokenizer tailored for the original GPT. It ensures that the model receives input in the expected format, efficiently tokenizing text into a mix of whole words and subword units, providing a balance between granularity and coverage. This tokenizer is crucial in allowing GPT to handle a wide range of text inputs and generate coherent and contextually relevant outputs."
      ],
      "metadata": {
        "id": "mFys_UjaB29M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is `OpenAIGPTModel` used for?\n",
        "\n",
        "### Use Cases for OpenAIGPTModel\n",
        "\n",
        "1. **Feature Extraction for Downstream Tasks**: The context-rich embeddings can be used as features for tasks like text classification, sentiment analysis, or named entity recognition.\n",
        "\n",
        "2. **Transfer Learning**: You can add a custom head on top of the model and fine-tune it for a specific task, leveraging the pre-trained knowledge of GPT.\n",
        "\n",
        "3. **Embedding Visualization**: To understand and visualize semantic relationships between words or sentences in a high-dimensional space.\n",
        "\n",
        "### Fancy Embeddings?\n",
        "\n",
        "- Yes, in a way. The primary difference between these embeddings and simpler methods is the context-awareness. While methods like Word2Vec or GloVe give a single embedding for each word regardless of its usage, the `OpenAIGPTModel` provides embeddings that change based on the word's context in a sentence.\n",
        "\n",
        "### Comparison with Other Embedding Methods\n",
        "\n",
        "1. **Static Embeddings (Word2Vec, GloVe)**:\n",
        "    - Produce a single, fixed embedding for each word.\n",
        "    - Don't account for polysemy (words with multiple meanings).\n",
        "    - Computationally less expensive.\n",
        "\n",
        "2. **Contextual Embeddings (ELMo)**:\n",
        "    - Produce different embeddings for words based on their context.\n",
        "    - Use LSTM-based architectures, so they consider sequential information.\n",
        "\n",
        "3. **Transformer-based Embeddings (BERT, GPT)**:\n",
        "    - Also produce context-aware embeddings but leverage the Transformer architecture.\n",
        "    - Typically have a higher capacity due to more parameters and layers.\n",
        "    - More computationally intensive but often achieve state-of-the-art performance.\n",
        "\n",
        "In conclusion, the `OpenAIGPTModel` provides a method to obtain context-aware embeddings from a Transformer architecture. While it's more computationally expensive than traditional embedding methods, it captures nuances and relationships in the data more effectively, making it valuable for a variety of NLP tasks."
      ],
      "metadata": {
        "id": "zq5fstBJienc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the use cases\n",
        "\n",
        "\n",
        "### Step 1: Understanding the Use Cases\n",
        "\n",
        "1. **Feature Extraction for Downstream Tasks**: This involves taking pre-trained models like GPT and using them to obtain embeddings or representations of the data. These embeddings serve as \"features\" which can then be used by another model or algorithm for a specific task.\n",
        "  \n",
        "2. **Transfer Learning**: This is the process of taking a pre-trained model and fine-tuning it on a smaller, task-specific dataset. By doing so, the model not only retains the knowledge it gained during its initial pre-training but also learns the specifics of the new task.\n",
        "\n",
        "3. **Embedding Visualization**: This is more about understanding and interpretation. Once embeddings are obtained, they can be visualized (often after dimensionality reduction) to understand the semantic relationships in the data.\n",
        "\n",
        "### Step 2: Categorizing Our Discussions\n",
        "\n",
        "1. **Using GPT for Embeddings (No Fine-tuning)**: This is purely \"Feature Extraction for Downstream Tasks\". We are using GPT's knowledge to obtain a representation of the data which can then be used in various ways, but we're not updating GPT's weights.\n",
        "\n",
        "2. **Fine-tuning GPT for a Specific Task (e.g., Sentiment Classification)**: This primarily falls under \"Transfer Learning\". We're adjusting GPT's pre-trained weights to better suit our specific task. However, one could argue that the embeddings we obtain during this process (from the last hidden layer before the custom head) are also a form of \"Feature Extraction for Downstream Tasks\", since they are used as features for the sentiment classification task. But the key distinction is that these embeddings are dynamic and are being refined as we fine-tune the model.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Both scenarios we discussed relate to \"Feature Extraction for Downstream Tasks\" but in slightly different ways:\n",
        "\n",
        "- When using GPT without fine-tuning, we're statically extracting features based on the model's pre-trained knowledge.\n",
        "  \n",
        "- When fine-tuning GPT, while we are indeed leveraging the model's embeddings (features) for our specific task, the process primarily aligns with \"Transfer Learning\" since we're updating the model's weights based on our task-specific data. However, since these refined embeddings are used for the task, it also has an element of \"Feature Extraction for Downstream Tasks\"."
      ],
      "metadata": {
        "id": "DmBH84Ly6Lxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `OpenAIGPTModel` in action"
      ],
      "metadata": {
        "id": "x0M46yJ0aZy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMGB3-bDUYdJ",
        "outputId": "f318b39b-541a-4c60-f705-c1b14b1733ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import OpenAIGPTModel, OpenAIGPTTokenizer"
      ],
      "metadata": {
        "id": "K3_F9ADEachF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'openai-gpt'\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "model = OpenAIGPTModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfMVM2XWanq2",
        "outputId": "a987989f-a2e9-4cba-e77b-738cadb35393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmd7TkTsTszz"
      },
      "outputs": [],
      "source": [
        "input_text = \"Lets see some embeddings\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DccSfNmmT9uk",
        "outputId": "ead23db4-7262-4770-8830-60921ea0b358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_states[0][1][:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsRqk7RHUARf",
        "outputId": "0e8ad658-05a3-4fd5-954d-eafefb206b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-5.4339e-01,  2.6350e-01,  6.4532e-02,  2.2958e-01,  1.6679e-01,\n",
              "        -2.8019e-01, -3.6579e-01, -6.6992e-01, -2.7126e-02,  6.8713e-01,\n",
              "         1.6432e-01,  5.7140e-01,  3.9758e-01,  4.1600e-01,  5.8372e-01,\n",
              "         9.1655e-01, -2.4924e-01, -2.4103e-01,  4.5438e-02,  6.3906e-01,\n",
              "         4.3120e-01, -1.4009e-01, -1.0626e-01, -7.7060e-01, -1.4231e-01,\n",
              "         2.3918e-01,  2.1953e-01, -2.9330e-01, -2.5660e-01, -1.3852e-01,\n",
              "         1.7396e-01, -4.4185e-01,  2.9353e-01, -6.9858e-01, -6.0446e-02,\n",
              "         3.6202e-02,  1.4981e-01,  1.1931e-01,  1.9105e-01, -2.1720e-01,\n",
              "         1.3792e-01,  2.9405e-01, -2.7587e-01,  6.7012e-02,  6.2635e-01,\n",
              "         2.1303e-01, -1.7063e-04,  6.7428e-02, -9.0618e-01, -9.7202e-01,\n",
              "         3.6795e-01, -4.3658e-01,  1.7653e-01,  2.5039e-02,  1.0016e-01,\n",
              "         1.5063e-01,  1.5239e-01, -1.6045e-02,  7.8693e-02,  1.1535e-01,\n",
              "         5.6031e-02,  7.3233e-01, -1.7351e-01, -4.6189e-01, -1.9365e-01,\n",
              "         3.2181e-02, -4.7779e-03, -3.9645e-01,  2.1105e-01, -1.4744e+00,\n",
              "         2.7791e-01,  2.5228e-01,  9.7716e-01,  8.5114e-01,  1.1186e+00,\n",
              "        -6.2909e-02,  1.2210e-01, -5.5332e-01,  7.9082e-01, -3.1003e-01,\n",
              "        -1.0503e-01, -1.4886e+00,  3.7013e-03,  3.6481e-02,  2.3774e-01,\n",
              "         2.9991e-01, -1.4955e+00, -6.8833e-01, -1.0836e+00, -7.5221e-01,\n",
              "         4.3751e-01,  2.3273e-01, -4.2941e-01, -4.3607e-01,  3.6994e-01,\n",
              "        -8.2389e-01,  6.4081e-01, -2.0772e-01, -2.4430e-01,  3.0446e-02],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case 1: Feature Extraction for Downstream Tasks"
      ],
      "metadata": {
        "id": "F1JaZwjIjeQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n",
        "\n",
        "import torch\n",
        "from transformers import OpenAIGPTModel, OpenAIGPTTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RGD5egijvXd",
        "outputId": "6e23a5d1-e336-4ca6-c61b-d5112ae74994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'openai-gpt'\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "model = OpenAIGPTModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmZXDZnOkKN6",
        "outputId": "1397e9da-3869-4081-cdd3-27f8884bb618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Use an existing token as the padding token\n",
        "tokenizer.pad_token = tokenizer.unk_token\n"
      ],
      "metadata": {
        "id": "LYG7JSIpl6hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"I love this!\", \"This is amazing.\", \"Not a fan of this.\", \"This is mediocre.\", \"Absolutely love it!\"]\n",
        "labels = [1, 1, 0, 0, 1]"
      ],
      "metadata": {
        "id": "6WrTnWHOkPm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=50)\n",
        "        return tokens.input_ids.squeeze(), torch.tensor(label)"
      ],
      "metadata": {
        "id": "oXWuu-85kW2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, gpt_model):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.gpt = gpt_model\n",
        "        self.linear = nn.Linear(768, 1)  # 768 is the size of GPT embeddings\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.gpt(input_ids).last_hidden_state\n",
        "        embeddings = embeddings.mean(dim=1)  # Average embeddings of all tokens to get a single vector for the sentence\n",
        "        output = self.linear(embeddings)\n",
        "        return torch.sigmoid(output).view(-1, 1)"
      ],
      "metadata": {
        "id": "kLqPQQqQkjSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "classifier = SentimentClassifier(model)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "# DataLoader\n",
        "dataset = SentimentDataset(sentences, labels, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for input_ids, label in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = classifier(input_ids).squeeze(-1)\n",
        "        print(output.shape)\n",
        "        print(label.shape)\n",
        "\n",
        "\n",
        "\n",
        "        loss = criterion(output, label.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B55KDoDBkrgI",
        "outputId": "9c161181-6ba6-4555-ee7e-b02de5d076f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 1/10, Loss: 3.1351914405822754\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 2/10, Loss: 1.4370181560516357\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 3/10, Loss: 0.6105945110321045\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 4/10, Loss: 1.0912686586380005\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 5/10, Loss: 0.5456282496452332\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 6/10, Loss: 1.1690982580184937\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 7/10, Loss: 0.6408752799034119\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 8/10, Loss: 0.9357937574386597\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 9/10, Loss: 0.8101611733436584\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "Epoch 10/10, Loss: 0.8229396343231201\n",
            "CPU times: user 1min 13s, sys: 6.42 s, total: 1min 19s\n",
            "Wall time: 1min 30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in classifier.parameters())\n",
        "print(f\"Total parameters in the model: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bORxhv_uQVR",
        "outputId": "e3baa7c1-dd41-476e-9df6-f8f51030d9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters in the model: 116535553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A simple text classification task\n",
        "\n",
        "Let's choose the first use case: \"Feature Extraction for Downstream Tasks\" and demonstrate it with a simple text classification task. We'll use the embeddings from the `OpenAIGPTModel` as features to classify sentences into two hypothetical categories: positive and negative sentiment.\n",
        "\n",
        "### Step 1: Install and Import Necessary Libraries\n",
        "\n",
        "Assuming you're in a Colab or Jupyter environment:\n",
        "\n",
        "```python\n",
        "!pip install transformers torch\n",
        "\n",
        "import torch\n",
        "from transformers import OpenAIGPTModel, OpenAIGPTTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "```\n",
        "\n",
        "### Step 2: Load the Tokenizer and Model\n",
        "\n",
        "```python\n",
        "model_name = 'openai-gpt'\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "model = OpenAIGPTModel.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "### Step 3: Create a Dataset\n",
        "\n",
        "For simplicity, let's assume we have the following small dataset of sentences and their corresponding sentiments (1 for positive and 0 for negative):\n",
        "\n",
        "```python\n",
        "sentences = [\"I love this!\", \"This is amazing.\", \"Not a fan of this.\", \"This is mediocre.\", \"Absolutely love it!\"]\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "```\n",
        "\n",
        "We'll define a PyTorch `Dataset` to manage our data:\n",
        "\n",
        "```python\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=50)\n",
        "        return tokens.input_ids.squeeze(), torch.tensor(label)\n",
        "```\n",
        "\n",
        "### Step 4: Create a Classifier using GPT Embeddings\n",
        "\n",
        "We'll add a simple linear layer on top of the GPT embeddings to classify the sentences.\n",
        "\n",
        "```python\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, gpt_model):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.gpt = gpt_model\n",
        "        self.linear = nn.Linear(768, 1)  # 768 is the size of GPT embeddings\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.gpt(input_ids).last_hidden_state\n",
        "        embeddings = embeddings.mean(dim=1)  # Average embeddings of all tokens to get a single vector for the sentence\n",
        "        output = self.linear(embeddings)\n",
        "        return torch.sigmoid(output)\n",
        "```\n",
        "\n",
        "### Step 5: Train the Classifier\n",
        "\n",
        "Now, we'll train this classifier on our small dataset.\n",
        "\n",
        "```python\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "classifier = SentimentClassifier(model)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "# DataLoader\n",
        "dataset = SentimentDataset(sentences, labels, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for input_ids, label in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = classifier(input_ids).squeeze()\n",
        "        loss = criterion(output, label.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
        "```\n",
        "\n",
        "This example demonstrates how to use the context-rich embeddings from the `OpenAIGPTModel` for a downstream task like sentiment classification. While the dataset is artificially small for demonstration purposes, in real-world applications, you'd apply a similar approach to much larger datasets for more robust and accurate results."
      ],
      "metadata": {
        "id": "el185KMsjoJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why did training on a tiny dataset take so long?\n",
        "\n",
        "### Step 1: Model Complexity\n",
        "\n",
        "**Factor**: The architecture of the model itself can impact training time. The `OpenAIGPTModel` is a transformer-based model which, even though you're only using it for feature extraction and not for generating sequences, can be computationally intensive due to its multiple layers and attention mechanisms.\n",
        "\n",
        "**Diagnosis**: Check the number of parameters in the `SentimentClassifier`:\n",
        "\n",
        "```python\n",
        "total_params = sum(p.numel() for p in classifier.parameters())\n",
        "print(f\"Total parameters in the model: {total_params}\")\n",
        "```\n",
        "\n",
        "### Step 2: Data Processing\n",
        "\n",
        "**Factor**: Tokenization and batching can also take up time, especially if you're not using optimized data loading techniques.\n",
        "\n",
        "**Diagnosis**:\n",
        "1. Time the tokenization process separately to see how long it takes.\n",
        "2. Time the data loading from the `DataLoader` in isolation to see if batching is slowing things down.\n",
        "\n",
        "### Step 3: Training Loop Operations\n",
        "\n",
        "**Factor**: Operations inside the training loop, like the forward pass, backward pass, loss computation, and optimizer step, all contribute to the training time.\n",
        "\n",
        "**Diagnosis**:\n",
        "1. Time each operation inside the training loop separately to understand which operation is the most time-consuming.\n",
        "2. Check for any unnecessary computations or operations inside the loop.\n",
        "\n",
        "### Step 4: Hardware Constraints\n",
        "\n",
        "**Factor**: The type of hardware being used can significantly impact training times. Training on a CPU will generally be slower than training on a GPU, especially for deep learning models.\n",
        "\n",
        "**Diagnosis**:\n",
        "1. Check if you're training on a GPU. If you're using PyTorch, you can confirm with `torch.cuda.is_available()`.\n",
        "2. If you're on a GPU, check its utilization. Sometimes, the GPU isn't fully utilized due to bottlenecks elsewhere (like data loading).\n",
        "\n",
        "### Step 5: External Factors\n",
        "\n",
        "**Factor**: Other processes running on your machine can slow down training. If your CPU or GPU is being utilized by other tasks, it can impact the performance of your training process.\n",
        "\n",
        "**Diagnosis**:\n",
        "1. Monitor CPU and GPU usage to see if they're being heavily used by processes other than your training script.\n",
        "2. If you're using a shared environment (like some cloud platforms), there might be constraints imposed on resources.\n",
        "\n",
        "### Step 6: Optimizations\n",
        "\n",
        "**Actions**:\n",
        "1. If data loading is a bottleneck, consider using `torch.utils.data.DataLoader` with the `num_workers` parameter set to a value greater than 1 to enable parallel loading of data.\n",
        "2. If the model's computations are the bottleneck, consider using a simpler architecture for preliminary tests or reduce the number of layers/parameters in the model.\n",
        "3. Ensure that all tensors and the model are moved to the GPU (if available) for faster computation.\n",
        "\n",
        "By systematically diagnosing each step, you can pinpoint where the majority of the time is being spent and then take appropriate actions to optimize the training process."
      ],
      "metadata": {
        "id": "xvSAfK8guels"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Size and Pre-training\n",
        "\n",
        "### Step 1: Model Size and Pre-training\n",
        "\n",
        "**Factor**: The GPT model, with its 100 million parameters, is undoubtedly a large model.\n",
        "\n",
        "**Clarification**: Yes, the model you've loaded from Hugging Face is pre-trained. Pre-training means the model has already been trained on a large dataset (like web text) to learn a language representation. However, when you're fine-tuning it on your dataset (`sentences`), you're adjusting these pre-trained weights to better suit your specific task. Even though it's pre-trained, the forward and backward passes during training still involve computations on all these 100 million parameters.\n",
        "\n",
        "### Step 2: Forward and Backward Passes\n",
        "\n",
        "**Factor**: Every epoch involves a forward pass (computing predictions) and a backward pass (computing gradients) for every batch. For models with many parameters, like GPT, these operations are computationally intensive.\n",
        "\n",
        "### Step 3: Optimizer Step\n",
        "\n",
        "**Factor**: The `optimizer.step()` updates the model's weights. For large models, this can also be time-consuming.\n",
        "\n",
        "### Step 4: Data Loading and Processing\n",
        "\n",
        "**Factor**: While your dataset is small, tokenization and data loading steps can still introduce some overhead, especially if the tokenization process isn't optimized.\n",
        "\n",
        "### Step 5: Hardware Constraints\n",
        "\n",
        "**Factor**: If you're running the training on a CPU, it will be considerably slower than on a GPU. Even if you are on a GPU, the specific type and its available memory can affect the speed.\n",
        "\n",
        "### Step 6: Other Overheads\n",
        "\n",
        "**Factor**: The `print` statements inside the loop, while seemingly innocuous, can introduce some overhead, especially if they're executed many times.\n",
        "\n",
        "### Step 7: Diagnosing the Time Consumption\n",
        "\n",
        "**Action**:\n",
        "\n",
        "1. Time different sections of your loop to see where the most time is spent. For instance:\n",
        "   \n",
        "   ```python\n",
        "   start = time.time()\n",
        "   output = classifier(input_ids).squeeze(-1)\n",
        "   forward_time = time.time() - start\n",
        "   \n",
        "   start = time.time()\n",
        "   loss.backward()\n",
        "   backward_time = time.time() - start\n",
        "   ```\n",
        "\n",
        "   This will give you a clearer picture of which operation is the most time-consuming.\n",
        "\n",
        "2. Check if you're running on GPU. If not, and you have access to one, move your model and data to the GPU. This can significantly accelerate the training process.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Given that the GPT model is large, even with a small dataset, the time taken per epoch isn't surprising. The forward and backward passes through a model with 100 million parameters can be time-consuming, especially if you're not using a high-end GPU. To reduce the training time, consider using a GPU if you aren't already, and profile different parts of your training loop to identify and address any specific bottlenecks."
      ],
      "metadata": {
        "id": "X5VUaPg2xQs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract embeddings with fine-tuning\n",
        "\n",
        "### Step 1: Understanding Pre-trained Models\n",
        "\n",
        "**Pre-trained Models**: These are models that have been trained on vast amounts of data, often on tasks like language modeling. The idea is that these models, through their extensive training, have learned a rich representation of the language. For example, GPT has been trained on vast amounts of text from the internet, so it has gained a deep understanding of language structure, semantics, and even some world knowledge.\n",
        "\n",
        "### Step 2: The \"Black Box\" Perception\n",
        "\n",
        "**Embeddings and Features**: When people refer to using models like GPT as a \"black box\" or a \"lego block,\" they often mean that they use the model to extract embeddings or features from the data without altering the model itself. In this scenario, the GPT model is kept static, and only its output (embeddings) is used as input for another model or task.\n",
        "\n",
        "### Step 3: Fine-tuning\n",
        "\n",
        "**Adjusting to Specific Tasks**: Instead of just extracting features and leaving the GPT model untouched, you can further train (or fine-tune) it on a specific task, like sentiment analysis. During this process, the weights of the GPT model get adjusted based on the new data and the specific task, allowing it to become more specialized.\n",
        "\n",
        "**Benefits**: Fine-tuning can lead to better performance on the specific task because the model not only leverages the general language knowledge it gained during pre-training but also the specific insights from the new data.\n",
        "\n",
        "### Step 4: An Analogy\n",
        "\n",
        "**Generalist to Specialist**: Imagine a medical doctor who has a broad understanding of medicine from medical school (akin to the GPT model's pre-training). This doctor then decides to specialize in cardiology and undergoes additional training (akin to fine-tuning). Now, this doctor is not just relying on the broad medical knowledge but is also an expert in cardiology. Similarly, by fine-tuning GPT, you're specializing it for a particular task.\n",
        "\n",
        "### Step 5: Potential Pitfalls\n",
        "\n",
        "**Overfitting**: Since models like GPT have a large number of parameters, they can easily overfit to a small dataset. This means that while they might perform exceptionally well on the training data, they might not generalize well to new, unseen data. Regularization techniques, early stopping, or even using a smaller version of the model can be strategies to combat this.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Using GPT for embeddings can be thought of as leveraging its broad knowledge of language. However, when you fine-tune GPT on a specific dataset, you're specializing it for a particular task, making it more adept at that task but at the risk of overfitting if not careful. This process harnesses both the general language understanding of the pre-trained model and the specific nuances of the new dataset to achieve better performance."
      ],
      "metadata": {
        "id": "sJDdxLWGykhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract embeddings without fine-tuning\n",
        "\n",
        "The GPT model can be used to extract embeddings without fine-tuning. In many applications, especially when the downstream dataset is small, using GPT (or similar models) just for embeddings can be quite beneficial. Let's break down this process step-by-step.\n",
        "\n",
        "### Step 1: Using GPT for Embeddings\n",
        "\n",
        "**Concept**: When using GPT purely for embeddings, you're leveraging the knowledge that GPT has gained during its pre-training phase. You feed your text into the model and obtain a representation (embedding) of the text from one of the model's layers.\n",
        "\n",
        "### Step 2: Choosing the Embedding Layer\n",
        "\n",
        "**Which Layer to Use?**:\n",
        "1. **Last Hidden State**: This is the most common choice. It represents the final output of the GPT model's layers for each token in the input.\n",
        "2. **Aggregated States**: You can average (or max) the last hidden states across all tokens to get a fixed-size representation for variable-length texts.\n",
        "\n",
        "### Step 3: Setting Up the Process\n",
        "\n",
        "1. **Load the GPT Model and Tokenizer**:\n",
        "   \n",
        "   ```python\n",
        "   model_name = 'openai-gpt'\n",
        "   tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "   model = OpenAIGPTModel.from_pretrained(model_name)\n",
        "   ```\n",
        "\n",
        "2. **Tokenize and Obtain Embeddings**:\n",
        "   \n",
        "   ```python\n",
        "   inputs = tokenizer(\"Your text here\", return_tensors=\"pt\")\n",
        "   with torch.no_grad():  # We don't need gradients for embedding extraction\n",
        "       outputs = model(**inputs)\n",
        "   embeddings = outputs.last_hidden_state\n",
        "   ```\n",
        "\n",
        "3. **Aggregate Embeddings (Optional)**:\n",
        "   \n",
        "   If you need a fixed-size representation, you can average the embeddings:\n",
        "   \n",
        "   ```python\n",
        "   aggregated_embedding = embeddings.mean(dim=1)\n",
        "   ```\n",
        "\n",
        "### Step 4: Using the Embeddings\n",
        "\n",
        "Once you have the embeddings:\n",
        "1. **Downstream Tasks**: You can feed these embeddings into another model (e.g., a classifier or regressor) depending on your task.\n",
        "2. **Similarity and Clustering**: The embeddings can be used to find similar texts or for clustering.\n",
        "\n",
        "### Step 5: Considerations\n",
        "\n",
        "1. **Static vs. Dynamic**: When used this way, GPT acts as a static feature extractor. The weights aren't updated based on the downstream task, which might be desirable if you have a small dataset or if you want to avoid the risk of overfitting.\n",
        "2. **Computational Efficiency**: Without the need for fine-tuning, using GPT purely for embeddings can be computationally more efficient during the downstream training process. This is because you'd only run the GPT model once for each text to get its embedding, rather than during each epoch of training.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Using GPT just for embeddings means leveraging its pre-trained knowledge as a static feature extractor. This approach can be beneficial for small datasets or when you want a computationally efficient method to obtain rich text representations without modifying the original GPT weights."
      ],
      "metadata": {
        "id": "yTWg0Kso5n0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debugging some problems\n",
        "\n",
        "### Step 1: The Problem\n",
        "\n",
        "**Problem Description**:\n",
        "During the training loop of a binary classification task using the `SentimentClassifier`, an error arose when processing batches of data, specifically when dealing with the last batch which contained only one item.\n",
        "\n",
        "**Symptoms**:\n",
        "1. Training worked fine for complete batches (of size 2), but threw an error when encountering the last batch (of size 1).\n",
        "2. The error indicated a shape mismatch, with the model's output tensor having an unexpected empty shape `torch.Size([])` for batches of size 1, while the label tensor had a shape `torch.Size([1])`.\n",
        "\n",
        "**Root Cause**:\n",
        "The classifier, when given input with a single sentence, produced an output with an inconsistent shape, compared to when it was given multiple sentences.\n",
        "\n",
        "### Step 2: The Solution\n",
        "\n",
        "**Solution Description**:\n",
        "The solution involved ensuring that the classifier's output always has a consistent shape regardless of the batch size. This was achieved by reshaping the output tensor to have at least two dimensions.\n",
        "\n",
        "**Actions Taken**:\n",
        "1. Modified the `forward` method in the `SentimentClassifier` to reshape its output using `.view(-1, 1)`. This ensured that the output always had a shape of `[batch_size, 1]`.\n",
        "   \n",
        "   Before: For batch sizes > 1, output shape was `[batch_size, 1]`. For a batch size of 1, output shape was `[]`.\n",
        "   \n",
        "   After: For all batch sizes, output shape became `[batch_size, 1]`.\n",
        "\n",
        "2. Simplified the training loop to squeeze the last dimension of the classifier's output using `.squeeze(-1)`, transforming its shape from `[batch_size, 1]` to `[batch_size]`.\n",
        "\n",
        "### Step 3: Summary\n",
        "\n",
        "In the process of training a binary classification model using batches of sentences, we encountered an inconsistency in tensor shapes when processing incomplete batches. The root of the issue was the classifier producing an inconsistent output shape when given single-item batches. This problem was addressed by ensuring that the classifier's output always had a consistent shape. The use of the `squeeze` method in the training loop was then streamlined, as the classifier's output was made consistent across all batch sizes. This resolved the shape-related errors and allowed for successful training."
      ],
      "metadata": {
        "id": "TvRBAEONt4AT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case 3: Visualizing Embeddings"
      ],
      "metadata": {
        "id": "Mxnretly7G--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization\n",
        "\n",
        "Visualizing embeddings can provide a lot of insight into the semantic relationships learned by the model. Let's break this down step-by-step:\n",
        "\n",
        "### Step 1: Obtain Embeddings\n",
        "\n",
        "**Objective**: First, we need to obtain embeddings for a set of sentences or words that we're interested in visualizing.\n",
        "\n",
        "```python\n",
        "model_name = 'openai-gpt'\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "model = OpenAIGPTModel.from_pretrained(model_name)\n",
        "\n",
        "sentences = [\"I love dogs\", \"Cats are wonderful\", \"Birds fly\", \"Fish swim\", \"I dislike rain\", \"Sunshine makes me happy\"]\n",
        "\n",
        "# Tokenize and obtain embeddings\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "```\n",
        "\n",
        "Here, we average the embeddings across tokens for each sentence to get a fixed-size representation.\n",
        "\n",
        "### Step 2: Dimensionality Reduction\n",
        "\n",
        "**Objective**: Embeddings are usually high-dimensional (e.g., 768 dimensions for GPT). For visualization, we need to reduce this to 2 or 3 dimensions.\n",
        "\n",
        "Common methods include:\n",
        "- PCA (Principal Component Analysis)\n",
        "- t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "Here's how you can use PCA:\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "```\n",
        "\n",
        "### Step 3: Visualization\n",
        "\n",
        "Now, with the reduced-dimensional embeddings, we can plot them:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = reduced_embeddings[:, 0]\n",
        "y = reduced_embeddings[:, 1]\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(x, y, marker='o')\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    plt.annotate(sentence, (x[i], y[i]))\n",
        "    \n",
        "plt.title(\"Visualization of Sentence Embeddings\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "In this example, sentences that are semantically closer to each other should ideally appear closer together in the plot. For instance, sentences about animals might cluster together, and those about feelings or weather might form another cluster. It's a powerful way to visually inspect the kind of semantic information captured by the model in its embeddings. Keep in mind that the effectiveness of the visualization can sometimes depend on the dimensionality reduction method and its parameters."
      ],
      "metadata": {
        "id": "YPZcaZZB7SPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The process of obtaining embeddings from the GPT model's outputs\n",
        "\n",
        "### Step 1: Understanding the Model's Output\n",
        "\n",
        "When we pass our tokenized inputs to the GPT model, the model returns an instance of `BaseModelOutput`. For the `OpenAIGPTModel`, this object primarily contains:\n",
        "\n",
        "- `last_hidden_state`: This is a tensor of shape `(batch_size, sequence_length, hidden_size)`. Each token in the input sequence is represented by a vector of size `hidden_size` (768 for GPT).\n",
        "\n",
        "For example, if you have a batch of 2 sentences and each sentence has 10 tokens (after tokenization), the shape of `last_hidden_state` would be `(2, 10, 768)`.\n",
        "\n",
        "### Step 2: Aggregating the Embeddings\n",
        "\n",
        "The objective now is to convert these embeddings from a 3D tensor to a 2D tensor where each sentence (or sequence) gets a single vector representation. One common way to do this is by averaging the embeddings of all tokens for each sentence.\n",
        "\n",
        "```python\n",
        "embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "```\n",
        "\n",
        "Here's the breakdown:\n",
        "\n",
        "- `outputs.last_hidden_state`: As explained, this accesses the tensor of shape `(batch_size, sequence_length, hidden_size)`.\n",
        "  \n",
        "- `.mean(dim=1)`: This computes the mean across the `sequence_length` dimension, effectively averaging the token embeddings for each sentence in the batch. The resulting shape is `(batch_size, hidden_size)`, or in our example `(2, 768)`.\n",
        "\n",
        "### Step 3: Converting to Numpy\n",
        "\n",
        "For many downstream tasks or libraries (like scikit-learn for PCA), it's often convenient to work with numpy arrays rather than PyTorch tensors. The conversion is straightforward:\n",
        "\n",
        "```python\n",
        "embeddings_numpy = embeddings.numpy()\n",
        "```\n",
        "\n",
        "This gives us a 2D numpy array with the shape `(batch_size, hidden_size)`.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "To summarize, when we pass our input data through the GPT model, we get a detailed representation for each token in the input. By averaging these token-wise representations, we obtain a single vector for each sequence or sentence in our input. This vector is a high-dimensional embedding that captures the semantic essence of the input sentence. By visualizing or using these embeddings in downstream tasks, we leverage the rich semantic information encoded by the GPT model."
      ],
      "metadata": {
        "id": "sS9o9uBg9sTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `OpenAIGPTLMHeadModel`"
      ],
      "metadata": {
        "id": "BY5lhQGFAU-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `OpenAIGPTLMHeadModel` in action\n",
        "\n",
        "The `OpenAIGPTLMHeadModel` is a version of the GPT-1 model that has an additional linear layer (the \"LM head\") on top of the base model, which makes it suitable for language modeling tasks. Here's how you can use it:\n",
        "\n",
        "### Step 1: Import Necessary Libraries and Load the Model\n",
        "\n",
        "First, we'll need to load the model and its corresponding tokenizer.\n",
        "\n",
        "```python\n",
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
        "\n",
        "model_name = 'openai-gpt'\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "### Step 2: Tokenization\n",
        "\n",
        "Now, let's tokenize a sample sentence.\n",
        "\n",
        "```python\n",
        "input_text = \"The quick brown fox\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "```\n",
        "\n",
        "### Step 3: Generating Text\n",
        "\n",
        "To generate text using the model, you can use the `generate` method.\n",
        "\n",
        "```python\n",
        "output = model.generate(**inputs, max_length=50, num_return_sequences=1, temperature=0.7)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "```\n",
        "\n",
        "This code snippet will extend the input_text (\"The quick brown fox\") with additional words/phrases, essentially continuing the sentence.\n",
        "\n",
        "### Step 4: Understanding the Output\n",
        "\n",
        "The `generate` method has several parameters, including:\n",
        "- `max_length`: The maximum length of the sequence to be generated.\n",
        "- `num_return_sequences`: The number of sequences to be generated.\n",
        "- `temperature`: This influences the randomness of the output. Lower values make the output more deterministic (and potentially more repetitive), while higher values make it more random.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The `OpenAIGPTLMHeadModel` allows you to perform language modeling tasks with the original GPT. By using the `generate` method, you can obtain sequences that continue from a given input, demonstrating the model's ability to understand and produce coherent and contextually relevant text. This can be used for a variety of applications, from chatbots to content generation tools."
      ],
      "metadata": {
        "id": "b5Abq4PDAXIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
        "\n",
        "model_name = 'openai-gpt'\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "8141cf8bb2e34e3198354abcf1210e7c",
            "a65c6020a6e64bb5beec396af5c4a7f1",
            "bac44a0ada234a09b6754e2ff24e6c9c",
            "1c1c2d7c0a804420907591e45c016464",
            "a3b061505e3046f5aadee85d6fec6e39",
            "39f2bc400c90404a8a65cd34c310ece7",
            "e9974d3982444aaab4190365c9bd8195",
            "ba11bc8db5f34ca78855422a251db1b1",
            "49692744d03a473dbb48864e9fdd8064",
            "31895ac76299437ebe3ab19246371552",
            "aca72e2fcbc4496ab4a3da664512449b"
          ]
        },
        "id": "cR-Gg_-pAhaz",
        "outputId": "7fbbdf88-04f5-40bc-f47f-b7a49a2a891c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8141cf8bb2e34e3198354abcf1210e7c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The quick brown fox\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "yuTRBy0SAmHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**inputs, max_length=200, num_return_sequences=1, temperature=0.7)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyHhBgj7ApGd",
        "outputId": "63ced7fa-1636-41c0-e07f-2a557cc40238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quick brown fox. \n",
            " \" i'm sorry, \" i said. \" i didn't mean to upset you. \" \n",
            " \" it's okay, \" he said. \" i'm just glad you're here. \" \n",
            " \" i'm glad i'm here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Model's Behavior\n",
        "\n",
        "The repetitive nature of the output you observed is a known behavior of language models, especially when generating long sequences. Let's analyze this step-by-step:\n",
        "\n",
        "### Step 1: Understanding the Model's Behavior\n",
        "\n",
        "**1. Limited Context Window**: GPT-1 and even its successors like GPT-2 and GPT-3 have a fixed context window (the number of tokens they can \"see\" or consider at once). If a generated text surpasses this window, the model can lose track of the broader context.\n",
        "\n",
        "**2. Greedy Nature**: When the model is unsure about the next token to generate, especially in an open-ended generation task, it can fall back to generating safe and commonly seen patterns, leading to repetition.\n",
        "\n",
        "### Step 2: The Role of the `temperature` Parameter\n",
        "\n",
        "The `temperature` parameter in the `generate` function influences the randomness of the model's outputs:\n",
        "- **Higher values** (e.g., close to 1 or above): The model's outputs become more random.\n",
        "- **Lower values** (e.g., close to 0): The model's outputs become more deterministic and can often be repetitive, as the model tends to choose the most likely next token.\n",
        "\n",
        "In your example, a `temperature` of 0.7 is a moderate value, but sometimes even that can lead to repetitive outputs, especially if the model enters a loop of generating a sequence it deems probable.\n",
        "\n",
        "### Step 3: Potential Solutions\n",
        "\n",
        "**1. Adjusting `temperature`**: You can experiment by increasing the temperature slightly to introduce more randomness into the generation.\n",
        "\n",
        "**2. Using a Stop Criterion**: Implement a mechanism to detect repetition and halt the generation process or steer it in a different direction.\n",
        "\n",
        "**3. Post-processing**: After generation, post-process the text to remove or reduce repetitive sequences.\n",
        "\n",
        "**4. Prompt Engineering**: Craft your prompts carefully. Sometimes, the way a prompt is phrased can guide the model into generating more diverse content.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Repetitiveness in the output of language models like GPT-1 is a known challenge. It arises due to the model's inherent nature and the balance it tries to strike between being coherent and being creative. While there's no guaranteed fix, the strategies mentioned above can help in mitigating such issues."
      ],
      "metadata": {
        "id": "p7_82IL4CPGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**inputs, max_length=200, num_return_sequences=1, temperature=1.0)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2htDOjbCZKI",
        "outputId": "884a6ed5-86a3-4c46-9f55-66af01f2e723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quick brown fox. \n",
            " \" i'm sorry, \" i said. \" i didn't mean to upset you. \" \n",
            " \" it's okay, \" he said. \" i'm just glad you're here. \" \n",
            " \" i'm glad i'm here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too. \" \n",
            " \" i'm glad you're here, too.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The generation process in terms of time\n",
        "\n",
        "The time taken during the generation process can be influenced by several factors. Let's break down the possible contributors to the time taken, step-by-step:\n",
        "\n",
        "### Step 1: Model Size and Complexity\n",
        "\n",
        "**GPT-1 Model Complexity**: Even though GPT-1 isn't as large as its successors, it's still a substantial model with over 100 million parameters.\n",
        "\n",
        "1. **Forward Pass**: When generating text, the model does a forward pass through all its layers for each token it predicts.\n",
        "2. **Autoregressive Nature**: GPT models generate text one token at a time in an autoregressive manner. After predicting one token, it's fed back into the model to predict the next token, and this process is repeated until the desired output length is reached.\n",
        "\n",
        "### Step 2: Hardware Limitations\n",
        "\n",
        "**GPU Acceleration**: GPT models, given their size, benefit significantly from GPU acceleration.\n",
        "\n",
        "1. **Colab's GPU**: If you're using a free tier of Google Colab, you might not always get the most powerful GPU, and sometimes there might be throttling.\n",
        "2. **Data Transfer**: Transferring data between the CPU and GPU can introduce overhead.\n",
        "\n",
        "### Step 3: Decoding Strategy\n",
        "\n",
        "**Generation Mechanism**: The method used to generate text can also impact the time taken.\n",
        "\n",
        "1. **Greedy Decoding**: This is relatively fast as it picks the most probable next token.\n",
        "2. **Beam Search**: Slower than greedy decoding, as it keeps track of multiple sequences (beams) and expands upon all of them.\n",
        "3. **Top-k and Top-p Sampling**: Introduce randomness in the generation by sampling from the top-k tokens or tokens that have a cumulative probability above a threshold p. The sampling methods can also add some overhead.\n",
        "\n",
        "### Step 4: External Overheads\n",
        "\n",
        "**Libraries and Frameworks**: The underlying implementation in libraries (like Hugging Face's Transformers) and deep learning frameworks (like TensorFlow or PyTorch) can introduce some overhead, although this is usually minimal compared to the actual model computation.\n",
        "\n",
        "**Colab Environment**: There's inherent overhead in running code in a cloud-based environment like Colab, especially if there are other tasks running simultaneously or if the server is experiencing high traffic.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The time taken for generating text using GPT models in a Colab environment is a cumulative result of the model's size and complexity, the hardware's capabilities, the chosen decoding strategy, and potential external overheads. For faster generation times, one could consider using more powerful GPU environments or optimizing generation strategies."
      ],
      "metadata": {
        "id": "GpgAnXQqDVQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'openai-gpt'\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
        "model = OpenAIGPTModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMCR4ehM7b62",
        "outputId": "f353cf12-1b44-4d37-bfd4-3a45c2ab31fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ],
      "metadata": {
        "id": "SAbaGwMB7tUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"I love dogs\", \"Cats are wonderful\", \"Birds fly\", \"Fish swim\", \"I dislike rain\", \"Sunshine makes me happy\", \"stormy daniels\", \"loud drums\"]\n",
        "\n",
        "# Tokenize and obtain embeddings\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "embeddings = outputs.last_hidden_state.mean(dim=1).numpy()"
      ],
      "metadata": {
        "id": "--TmTlFA7hWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K_hMEIs8n7S",
        "outputId": "d98e776e-68ab-41d8-af1e-c6e6befb6757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OSoPW-J8t7s",
        "outputId": "2151920e-2671-4d1d-8b9f-61898cdf4fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.modeling_outputs.BaseModelOutput"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "UpXI_vzb7yOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = reduced_embeddings[:, 0]\n",
        "y = reduced_embeddings[:, 1]\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(x, y, marker='o')\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    plt.annotate(sentence, (x[i], y[i]))\n",
        "\n",
        "plt.title(\"Visualization of Sentence Embeddings\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "TOChrmpX72P1",
        "outputId": "8b57f8f2-19f3-4ff6-8b1b-5a0eaf314449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2"
      ],
      "metadata": {
        "id": "HCuLDgvN_Ls8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The transition from GPT to GPT-2\n",
        "\n",
        "The transition from GPT to GPT-2 saw several significant improvements and refinements. Let's break these down step-by-step:\n",
        "\n",
        "### Step 1: Model Size\n",
        "\n",
        "**GPT**:\n",
        "- GPT had around 110 million parameters.\n",
        "\n",
        "**GPT-2**:\n",
        "- The largest variant of GPT-2 boasts a whopping 1.5 billion parameters.\n",
        "- OpenAI also released smaller versions with 125 million, 355 million, and 774 million parameters, respectively.\n",
        "\n",
        "**Implication**: A larger model size generally allows the model to capture more intricate patterns and relationships in the data. With GPT-2, the increased model size was one of the primary reasons for its enhanced performance in understanding and generating coherent and contextually relevant text.\n",
        "\n",
        "### Step 2: Training Data and Scale\n",
        "\n",
        "**GPT**:\n",
        "- GPT was trained on the BooksCorpus dataset.\n",
        "\n",
        "**GPT-2**:\n",
        "- GPT-2 was trained on a larger dataset called WebText, which was extracted from web pages and is considerably larger and more diverse than BooksCorpus.\n",
        "\n",
        "**Implication**: Training on a more diverse dataset allows the model to capture a wider range of language styles, topics, and nuances.\n",
        "\n",
        "### Step 3: Model Architecture\n",
        "\n",
        "While the underlying architecture of both GPT and GPT-2 is based on the Transformer, GPT-2 has refinements in terms of the number of attention heads, layers, etc., especially in its larger variants.\n",
        "\n",
        "### Step 4: Zero-shot, Few-shot, and Many-shot Learning\n",
        "\n",
        "With GPT-2, OpenAI showcased the model's ability to perform tasks in:\n",
        "- **Zero-shot learning**: The model performs a task without any specific examples.\n",
        "- **Few-shot learning**: The model is given a few examples to infer the task.\n",
        "- **Many-shot learning**: The model is provided with many examples to understand the task.\n",
        "\n",
        "This demonstrated GPT-2's capability to generalize from limited data in a way that GPT wasn't specifically highlighted for.\n",
        "\n",
        "### Step 5: Challenges and Controversies\n",
        "\n",
        "When GPT-2 was initially released, OpenAI expressed concerns about its potential misuse due to its ability to generate highly convincing fake news articles, impersonate others, or produce harmful content. As a result, OpenAI initially held back the release of the largest model, sharing only the smaller variants. This was a distinction from the GPT release.\n",
        "\n",
        "### Step 6: Fine-tuning and Adaptability\n",
        "\n",
        "GPT-2 showcased enhanced adaptability when fine-tuned on specific datasets. While both GPT and GPT-2 can be fine-tuned, GPT-2's larger capacity and training on a diverse dataset made it more effective in adapting to a wider range of tasks.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "GPT-2 was a significant leap from GPT in terms of model size, training data, and versatility. The advancements in GPT-2 set the stage for even more sophisticated language models, pushing the boundaries of what's possible in the domain of natural language understanding and generation. It highlighted the strengths of scaling up models and training on diverse data while also drawing attention to the potential ethical concerns of such powerful models."
      ],
      "metadata": {
        "id": "a3BPEDIN_pjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The evolution of the GPT architecture\n",
        "\n",
        "The distinction you're pointing out reflects the evolution of the GPT architecture and the different ways in which OpenAI approached the models. Let's break this down step-by-step:\n",
        "\n",
        "### Step 1: GPT-1 Model Variants\n",
        "\n",
        "**1. OpenAIGPTModel (Base)**:\n",
        "- This is the foundational model which outputs raw hidden-states without any specific head on top.\n",
        "- Primary use: As a building block for the other two variants and for tasks that need contextual embeddings (like sentence embeddings).\n",
        "\n",
        "**2. OpenAIGPTLMHeadModel (Head)**:\n",
        "- This has an additional language modeling head on top of the base GPT model.\n",
        "- Primary use: For tasks like text generation.\n",
        "\n",
        "**3. OpenAIGPTDoubleHeadsModel (DoubleHead)**:\n",
        "- This is a more specialized version with two heads: one for language modeling and another for a classification task.\n",
        "- Primary use: For tasks that combine text generation and classification, like in some conversational AI scenarios.\n",
        "\n",
        "### Step 2: GPT-2 Model Variants by Size\n",
        "\n",
        "For GPT-2, OpenAI shifted the focus more towards the size of the model. This was in line with their discovery that, given sufficient data and compute, larger models perform better across a wider range of tasks without task-specific architectures. The variants of GPT-2 are:\n",
        "\n",
        "**1. `gpt2-small`**:\n",
        "- The smallest version of GPT-2.\n",
        "\n",
        "**2. `gpt2-medium`**:\n",
        "- Intermediate-sized model, offering a balance between computational efficiency and performance.\n",
        "\n",
        "**3. `gpt2-large` and `gpt2-xl`**:\n",
        "- The largest publicly available models, delivering the highest performance but also requiring the most computational resources.\n",
        "\n",
        "### Step 3: The Rationale Behind the Approach\n",
        "\n",
        "**1. Simplicity and Generalization**: By focusing on model size, OpenAI aimed to create a single model that could excel across a variety of tasks without task-specific modifications. The larger the model, the better it performed on a range of benchmarks.\n",
        "\n",
        "**2. Ethical Concerns**: OpenAI initially did not release the largest models due to concerns about potential misuse. Over time, as they assessed the risks and benefits, they released increasingly larger models, but not the absolute largest ones.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "While GPT-1 had a few different model architectures tailored for specific tasks, GPT-2 moved towards a \"one size fits all\" (or rather, \"one architecture, different sizes\") approach. The idea was to leverage the power of larger models to handle a variety of tasks without the need for task-specific heads or modifications. This approach aligns with the broader trend in the deep learning community of using larger models and fine-tuning them for specific tasks rather than designing task-specific architectures."
      ],
      "metadata": {
        "id": "xEuwvqcFFgFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The different models available for GPT-2\n",
        "\n",
        "GPT-2 is the successor to GPT and brought about several refinements and improvements. Let's delve into the different models available for GPT-2 and their specifications step-by-step:\n",
        "\n",
        "### Step 1: Understanding GPT-2's Variants\n",
        "\n",
        "OpenAI released multiple sizes (or variants) of the GPT-2 model. The primary difference between these variants is the number of parameters (or the model's size).\n",
        "\n",
        "**GPT-2 Variants**:\n",
        "1. **`gpt2-small`**: This is the smallest version with 125 million parameters.\n",
        "2. **`gpt2-medium`**: A medium-sized model with 355 million parameters.\n",
        "3. **`gpt2-large`**: Larger with 774 million parameters.\n",
        "4. **`gpt2-xl`**: The largest GPT-2 model with a whopping 1.5 billion parameters.\n",
        "\n",
        "### Step 2: Use Cases Based on Model Size\n",
        "\n",
        "1. **Small and Medium Models**:\n",
        "   - **Pros**: Faster inference, less memory-intensive, suitable for edge devices or applications where response time is crucial.\n",
        "   - **Cons**: Might not capture the depth of semantic relationships as well as the larger models.\n",
        "   \n",
        "2. **Large and XL Models**:\n",
        "   - **Pros**: Better at capturing intricate language nuances, produce more fluent and coherent text.\n",
        "   - **Cons**: Computationally intensive, require more memory, can be overkill for simple tasks.\n",
        "\n",
        "### Step 3: Architectural Details\n",
        "\n",
        "Apart from the number of parameters, the architectural details like the number of attention heads, the number of layers, etc., also vary across these models. However, the core architecture, which is based on the Transformer, remains consistent across all sizes.\n",
        "\n",
        "### Step 4: Model Availability and Pre-trained Weights\n",
        "\n",
        "All these models are available in the Hugging Face model repository, and they come with pre-trained weights. This means they have already been trained on a large corpus of text (like web pages) and can be used out-of-the-box for various NLP tasks or fine-tuned on specific datasets.\n",
        "\n",
        "### Step 5: Specialized Variants\n",
        "\n",
        "Apart from the base GPT-2 models, there might be community-contributed models that have been fine-tuned for specific tasks or languages. These specialized variants can be found on the Hugging Face model hub.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "GPT-2 offers a range of model sizes to cater to different computational needs and tasks. While the core architecture remains consistent, the number of parameters varies, allowing users to make trade-offs between computational efficiency and model performance. As always, the choice of model size should be based on the specific application, the available computational resources, and the desired output quality."
      ],
      "metadata": {
        "id": "FUP-1hj1_OEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Cases for GPT-2\n",
        "\n",
        "GPT-2, with its enhanced capabilities compared to GPT-1, has a wide range of use cases. Let's break them down step-by-step:\n",
        "\n",
        "### Step 1: Text Generation\n",
        "\n",
        "Given its primary design as a language model, GPT-2 excels at generating coherent, diverse, and contextually relevant text.\n",
        "\n",
        "**Examples**:\n",
        "1. **Content Creation**: It can assist writers by generating articles, stories, or poetry.\n",
        "2. **Dialog Systems**: Used in chatbots and customer support automation to generate human-like responses.\n",
        "\n",
        "### Step 2: Transfer Learning\n",
        "\n",
        "GPT-2 can be fine-tuned on a specific dataset to adapt its vast knowledge to specific tasks or domains.\n",
        "\n",
        "**Examples**:\n",
        "1. **Specialized Chatbots**: Fine-tuning on medical or legal datasets for domain-specific responses.\n",
        "2. **Text Classification**: Although primarily a generative model, GPT-2 can be adapted for classification tasks like sentiment analysis.\n",
        "\n",
        "### Step 3: Text Completion\n",
        "\n",
        "GPT-2 can complete partial sentences or paragraphs, making it useful for predictive typing applications.\n",
        "\n",
        "**Examples**:\n",
        "1. **Code Completion**: Assisting programmers by predicting the next line of code.\n",
        "2. **Email and Writing Assistants**: Predicting the continuation of sentences for faster typing.\n",
        "\n",
        "### Step 4: Translation and Language Tasks\n",
        "\n",
        "While not its primary design, with appropriate prompts, GPT-2 can perform tasks like translation or text summarization.\n",
        "\n",
        "**Examples**:\n",
        "1. **Machine Translation**: Translating text between different languages.\n",
        "2. **Summarization**: Providing concise summaries of longer articles or documents.\n",
        "\n",
        "### Step 5: Question Answering\n",
        "\n",
        "GPT-2 can be used to answer questions based on a given context or its pre-trained knowledge.\n",
        "\n",
        "**Examples**:\n",
        "1. **Document-based QA**: Answering questions based on a provided document or article.\n",
        "2. **General QA**: Answering general knowledge questions.\n",
        "\n",
        "### Step 6: Text-based Gaming and Entertainment\n",
        "\n",
        "Given its narrative capabilities, GPT-2 can be used in interactive text-based games or storytelling platforms.\n",
        "\n",
        "**Examples**:\n",
        "1. **Interactive Stories**: Users can co-write stories with the model, choosing directions and getting generated content in response.\n",
        "2. **Text-based RPGs**: Role-playing games where the narrative is co-constructed by the player and the model.\n",
        "\n",
        "### Step 7: Educational and Research Tools\n",
        "\n",
        "GPT-2 can assist in educational scenarios and research.\n",
        "\n",
        "**Examples**:\n",
        "1. **Homework Helpers**: Assisting students in understanding complex topics or providing explanations.\n",
        "2. **Research Assistance**: Generating content or ideas in academic research.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "GPT-2, with its advanced capabilities, has a wide range of applications, from content generation to assisting in various professional tasks. Its flexibility and adaptability make it a versatile tool in the NLP domain. However, it's essential to use it responsibly, especially in tasks that can have ethical implications, such as content creation, due to the risk of generating misleading or false information."
      ],
      "metadata": {
        "id": "HsKbIJz0EDtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case 1: Text Generation\n",
        "\n",
        "Let's walk through the process of setting up and running GPT-2 (specifically the `gpt2-small` variant) for text generation on Google Colab:\n",
        "\n",
        "### Step 1: Install Necessary Libraries\n",
        "To utilize GPT-2, you'd typically use the Hugging Face's `transformers` library, which provides pre-trained models and tokenizers for a wide range of NLP tasks.\n",
        "\n",
        "```python\n",
        "!pip install transformers\n",
        "```\n",
        "\n",
        "### Step 2: Import Required Modules\n",
        "You'll need to import the model and tokenizer specific to GPT-2.\n",
        "\n",
        "```python\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "```\n",
        "\n",
        "### Step 3: Load the Pre-trained Model and Tokenizer\n",
        "\n",
        "```python\n",
        "model_name = \"gpt2-small\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "### Step 4: Generate Text\n",
        "\n",
        "Given an initial prompt, you can use the model to generate text. The `generate` method is a versatile tool that allows for various generation strategies.\n",
        "\n",
        "```python\n",
        "# Encode the input text to tensor\n",
        "input_text = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=1.0)\n",
        "\n",
        "# Decode the generated text back to readable text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "```\n",
        "\n",
        "### Step 5 (Optional): Tweak Generation Parameters\n",
        "\n",
        "You can experiment with various generation parameters:\n",
        "\n",
        "- `max_length`: Determines the maximum length of the generated text.\n",
        "- `num_return_sequences`: If you want multiple different continuations, you can increase this number.\n",
        "- `temperature`: A higher value (e.g., 1.0 or above) makes the output more random, while a lower value (e.g., 0.7 or below) makes it more deterministic.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Once you've set up the above code in a Google Colab notebook, you'll be able to generate text using GPT-2's `gpt2-small` variant. You can change the initial prompt or tweak the generation parameters to see different outputs. Remember, since GPT-2 is a powerful model, even the `gpt2-small` variant might take a few moments to generate the text, depending on the length and other parameters."
      ],
      "metadata": {
        "id": "P8KUIom2HGtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "5utZeh3cHgE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_Fu3vL9HSYx",
        "outputId": "a52a9984-6d49-41fb-ea2a-a2a40638f0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "BP9ZvSxwHU1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "# Move the model to the device\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659,
          "referenced_widgets": [
            "17cd912f0c77478c882f6fddbb191a5f",
            "fb6199a801e64673b657fa03a8d725d1",
            "e4b8a5e8d7484be597636f5edf24d4a5",
            "37609ed806604091bb3d978ef310a352",
            "ad74ef86c77a4c96965b988ba5d8d05c",
            "3271584b343c4ebba2db6fc9d07363ee",
            "cb0e472080654a0ca17f42ec2d68be60",
            "fdb448406f4d4feabbda0f0ca2716143",
            "d24932b526ea41a491a5911d5af45b41",
            "6f5d14a5b8ae4e47ad05bba8d8b3524d",
            "3ad4816662804119aa1594cdefabc89a",
            "1f45b05dec514df6b935e3fa50bf24b9",
            "3bb565ccd3b54184bb673a68c1670a07",
            "d7b1053804da4c2aa759c8b75f428728",
            "6874d8cae2ba4edd8f4e6dcc7a04d47c",
            "9b1f7c54878149b5ae5d2e7cdac97719",
            "e9cec6aad0dd40d5b1c689894865bfa7",
            "6d277d7ed4cd47c096ce814f6c25d0ae",
            "4c8fb586a82449e79356b2aecc605777",
            "0bf25767fb25445bb17924968905e8fe",
            "5f08f026b8024cdc8ebaedc3ec332c69",
            "3d6b05c39dad42a58910855fbf0f8559",
            "8b78de74dacc4defbb0bfde5cc1c8697",
            "4c5742a75ae94d7db2f22d0bbe042862",
            "8604830df60d4195b58c38a2c715b5bb",
            "7bf1da3efb6a4fb8bfefafca0f3f7b50",
            "dda94923e23e430aada03b207d8fadda",
            "9a8d02474bc84748abd6dd69857bc99b",
            "af46d674e1a749dcbee31ef3f90071c4",
            "668b42aad898461c9b57bba3124557bd",
            "17425ab6804045dfa1e58acfb001cf81",
            "23835132e2544828b027ffc71e9062e1",
            "6f31ffcae0554f1caa6fdded9566f97f",
            "90769f4a744247d3ae7355c34a0ea499",
            "9c54124f622045089adcb74115b46d2b",
            "78c25d5295dd4a08a5b5bb194a9718fd",
            "08a35c4c4fe74e93a608298f8194d7ff",
            "e104a4f60ef440cf839c92563ce0d821",
            "45e046dfb2bc453eb484ee6a3f35f8b7",
            "bdd1880d9f8342aaacaa8605844c7c84",
            "d2d06be84a36445c88db1c44b576d703",
            "d86741e7f6d045f284fcade6272ec21b",
            "c6fa9a2329704cc9818aa87612edf9c4",
            "bff37a13fa0c4ed5af904d07e48aa733",
            "f5869141077f4093bcd29946958a9e2d",
            "146ac458a5c74c7ba5b29cf4e9035671",
            "74617a48fafc4fef96962ccf3641c47e",
            "5d5b079711f94981be863fbfb2874a45",
            "e56ab3f8f0c44c35baab508f169f5782",
            "0e0591866a6941229fc2eb49b12336a8",
            "6b749a8afb834559a1ba414ee07dcdfe",
            "990e88d1778d47258567c77095c7c4ef",
            "529a2cc1ff3b41b1afd37b73da1b80f2",
            "83175d25607443ce88cfcf502770d54f",
            "b033bd878deb4f60b5d10a19aecc8e96"
          ]
        },
        "id": "LL5E9eWHHYgw",
        "outputId": "8394b881-4b84-4fb4-903e-002ff3ce53bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17cd912f0c77478c882f6fddbb191a5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f45b05dec514df6b935e3fa50bf24b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b78de74dacc4defbb0bfde5cc1c8697"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90769f4a744247d3ae7355c34a0ea499"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5869141077f4093bcd29946958a9e2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the padding token for the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# Encode the input text to tensor\n",
        "input_text = \"Once upon a time\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=100, return_attention_mask=True)\n",
        "\n",
        "# Move the inputs to the GPU\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Generate text with adjusted temperature\n",
        "output = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=200, temperature=1.0)\n",
        "\n",
        "# Decode the generated text back to readable text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz6ob7cnHaza",
        "outputId": "7aaf8372-e9b2-48a6-8a29-23f2c5906cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interesting applications of GPT-2 in a Colab\n",
        "\n",
        "GPT-2 is versatile and can be employed in various interesting applications. Here are a few use cases that might be intriguing to explore:\n",
        "\n",
        "### 1. **Story Completion or Expansion**\n",
        "Provide the beginning of a story and let GPT-2 complete it or expand on it. This can be a fun way to generate creative content and see how the model weaves narratives.\n",
        "\n",
        "### 2. **Q&A Sessions**\n",
        "While GPT-2 isn't specifically designed for question-answering like some other models, you can still pose questions to it and receive coherent (if not always accurate) responses. It's an interesting way to see how the model generates information based on its training data.\n",
        "\n",
        "### 3. **Content Augmentation**\n",
        "If you're working on a project that needs varied examples of textual content (for training data, perhaps), you can seed GPT-2 with prompts and generate diverse samples.\n",
        "\n",
        "### 4. **Paraphrasing**\n",
        "Provide a sentence and get GPT-2 to rephrase it. While it's not specifically a paraphrasing model, it can often provide different ways of expressing the same idea, which can be useful for content creation or data augmentation.\n",
        "\n",
        "### 5. **Code Comment Generation**\n",
        "Feed it a line of code, and see if GPT-2 can generate a relevant comment. This can be an engaging way to explore its capabilities in understanding and generating code-related content.\n",
        "\n",
        "### 6. **Mimicking Styles**\n",
        "Train GPT-2 (fine-tuning) on specific authors' works or specific genres and see if it can mimic that style when generating new content. This is a more advanced use case but can be very fascinating.\n",
        "\n",
        "### 7. **Interactive Chatbot**\n",
        "Create an interactive session where you can converse with GPT-2. Each response from the model can be used as context for the next, allowing for a back-and-forth dialogue.\n",
        "\n",
        "**Step-by-Step for a Chosen Use Case - Interactive Chatbot**:\n",
        "\n",
        "1. **Initialize the Model and Tokenizer**:\n",
        "   As done previously, initialize GPT-2 and its tokenizer.\n",
        "   \n",
        "2. **Set up an Interactive Loop**:\n",
        "   Create a loop where the user can input a message, the model generates a response, and then the user can reply to that, and so on.\n",
        "   \n",
        "3. **Maintain Context**:\n",
        "   To make the conversation flow naturally, maintain the chat history so that the model has context for its replies. This means appending each new message from the user to the conversation history before generating the model's response.\n",
        "   \n",
        "4. **Decoding the Model's Response**:\n",
        "   Use the tokenizer to decode the generated tokens into a coherent message.\n",
        "   \n",
        "5. **Display and Continue**:\n",
        "   Show the model's response and loop back to get the user's next message.\n",
        "\n",
        "This chatbot can be an entertaining way to interact with GPT-2 and get a sense of its capabilities and quirks in a conversational context."
      ],
      "metadata": {
        "id": "DYpgQh6oLO-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case: Chatbot\n",
        "\n",
        "Creating a chatbot using GPT-2 in a Colab environment can be broken down into the following steps:\n",
        "\n",
        "### Step 1: Initialization\n",
        "Load the necessary libraries, set up the GPT-2 model and its tokenizer.\n",
        "\n",
        "### Step 2: Interactive Loop\n",
        "Set up a loop for interaction where:\n",
        "- You input a message.\n",
        "- The model processes the context and generates a response.\n",
        "- The response is displayed.\n",
        "- The process repeats.\n",
        "\n",
        "### Step 3: Context Management\n",
        "In order to maintain the flow of the conversation, we'll append each user message and model response to a running chat history. This way, the model has the entire context of the conversation when generating its replies.\n",
        "\n",
        "Let's implement these steps.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-Step Code:\n",
        "\n",
        "**Step 1: Initialization**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Ensure GPU usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2-medium\"  # You can also use \"gpt2-small\" for quicker response times.\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "**Step 2 & 3: Interactive Loop with Context Management**\n",
        "\n",
        "```python\n",
        "chat_history = \"\"\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "    # Append user input to chat history\n",
        "    chat_history += f\"You: {user_input}\\n\"\n",
        "    # Tokenize and process input\n",
        "    input_tensor = tokenizer.encode(chat_history + \"\\n\", return_tensors='pt', truncation=True, max_length=1024).to(device)\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        response_tensor = model.generate(input_tensor, max_length=1024, pad_token_id=tokenizer.eos_token_id, temperature=1.0)\n",
        "    # Decode and display the model's response\n",
        "    response = tokenizer.decode(response_tensor[:, input_tensor.shape[-1]:][0], skip_special_tokens=True)\n",
        "    print(f\"GPT-2: {response}\")\n",
        "    # Append model response to chat history\n",
        "    chat_history += f\"GPT-2: {response}\\n\"\n",
        "    \n",
        "    # Optional: Limit the length of chat history to prevent out-of-memory issues\n",
        "    if len(chat_history) > 2000:\n",
        "        chat_history = chat_history.split(\"\\n\", 1)[-1]\n",
        "```\n",
        "\n",
        "Run this code in a Colab cell, and you should be able to have a conversation with the GPT-2 model! Remember, using a larger model variant (like \"gpt2-medium\" or \"gpt2-large\") will result in more coherent replies but might take longer to generate responses, especially if not using a GPU."
      ],
      "metadata": {
        "id": "e1uvD8L_LiLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Ensure GPU usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2-medium\"  # You can also use \"gpt2-small\" for quicker response times.\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "9fc905d677c24bc1a49ede52311baec5",
            "62f05cbd67844bd180c4929281e4e1f0",
            "8646a0f0da4b48bcbf15b780cc808c2d",
            "5177460348d9423a85005a46f0a0f4e2",
            "e630be3f5fea41e99dd2a4dced9ffbcb",
            "017497ee1fe84f0eb1537d8dcc74c2be",
            "169c48465a694732b48d13dedbe4157c",
            "e3a2dcf6c9f74a1ca187ffabeef2442a",
            "d8399f77de4b43f195b6f8e6d7d13ed2",
            "e317422e60454c8ca4ca50449ba3d820",
            "1bc010879c3d4420be0a0bdcd5ba4814",
            "68fc69c2bd4e4ee1ba7c1f2c06c14078",
            "b004f466e649451db9ef66724eccec4f",
            "acd2ab3d7b37499a9b39ead7a1a93cba",
            "b41fae4b6f824e718d94dfc130475071",
            "16a524e354534c3db34a654c1c82549e",
            "5fd5b8c0ce5b4ef48a60e29a28dbe2dc",
            "46e894c3fee746c4a3b7505c8676d259",
            "a916b0cc4c2d4663b2f4982562afe3bd",
            "7d4966ee9bc3449186ef5e18029e331c",
            "66e5962e7a924019a096cc8dde94df22",
            "9572e154a90b42ebb9cf40ec36129745",
            "e0759554d6cd4eaba770528538304357",
            "406ec85504d84edd911436f20c53a74e",
            "962ea5e09ac441e89f8352da48f2c1c9",
            "2c0eb383301b4f8282fa3368dbf1242b",
            "f2b8b83cc0db47489c587fc53f0d1bc0",
            "03082ac336a94b90a5aa8c72da56e214",
            "d91d9d64eecb46e8931e76e86b5ff83b",
            "cf14519e3b0d4debac19a092a4bc564b",
            "30563ffa6a9d423d9dd2ca6e96b5ddbc",
            "dd68abdd6588411784d9d72c00dc2cd5",
            "e60b8bbd87674ccfa360262f26778d70",
            "09f7311eb5e24543a80dbe365fc72c74",
            "3ded964ff9264d439d3f612045a2a27b",
            "d65ee7c4b8b34c26afa657a5d3f86291",
            "d901ba3e18304885a2df3a4d83257d5f",
            "eacb1eaf4cca4c5d8987d6b84517de0f",
            "30b6cce3f9224b79bda52a32e0f899f7",
            "8451e7baff73406a988b1a60958c824e",
            "7daa2ac6ea70467b876059de116d400e",
            "a52e13458d664254997bb61bba3e0160",
            "780b3b3016cd4694959829d6eca86679",
            "98e9ee3fd0b747bb83bdb4341edc1c46",
            "f2682a9ca8d04ed3bf878c0b0711efef",
            "e2855331553f40559b07858f9c45fec5",
            "93088a4ed7b24d2a88e02a059144684b",
            "8a8cbeb270dd4805acf512441e22c4e2",
            "fb1bf5f7b959472aa72c6ab25328f7b8",
            "0f5338c1653940b28085cce29aad844e",
            "9cabbfd2b23c4098be5620d354fb58fd",
            "c78b9fd4945c4b699f9977a0483860a7",
            "64762c9b3bf74da8b4dedb62561e34af",
            "58c2592333ec4e67acddffae2d1b7964",
            "1e1569aac240478296e07b03ea8aeb16"
          ]
        },
        "id": "piPrcnpqLniu",
        "outputId": "46e4cbcd-337c-4b30-c094-6c0e1ed944d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fc905d677c24bc1a49ede52311baec5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68fc69c2bd4e4ee1ba7c1f2c06c14078"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0759554d6cd4eaba770528538304357"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09f7311eb5e24543a80dbe365fc72c74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2682a9ca8d04ed3bf878c0b0711efef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = \"\"\n",
        "iterations = 5  # Number of iterations you want the chat to run\n",
        "\n",
        "for _ in range(iterations):\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "    # Append user input to chat history\n",
        "    chat_history += f\"You: {user_input}\\n\"\n",
        "    # Tokenize and process input\n",
        "    input_tensor = tokenizer.encode(chat_history, return_tensors='pt', truncation=True, max_length=1024).to(device)\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        response_tensor = model.generate(input_tensor, max_length=1024, pad_token_id=tokenizer.eos_token_id, temperature=1.0)\n",
        "\n",
        "    # Extract only the new tokens generated by the model\n",
        "    num_new_tokens = response_tensor.shape[1] - input_tensor.shape[1]\n",
        "    response = tokenizer.decode(response_tensor[0][-num_new_tokens:], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"GPT-2: {response}\")\n",
        "    # Append model response to chat history\n",
        "    chat_history += f\"GPT-2: {response}\\n\"\n",
        "\n",
        "    # Optional: Limit the length of chat history to prevent out-of-memory issues\n",
        "    if len(chat_history) > 2000:\n",
        "        chat_history = chat_history.split(\"\\n\", 1)[-1]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6imNF1zoLyYD",
        "outputId": "a4463f8c-7cf5-48b6-ff04-f6df5e29aff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: hello mr\n",
            "GPT-2: \n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "Mister: hello mr\n",
            "\n",
            "M\n",
            "You: yes don't just repeat my words\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1024, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2: ister\n",
            "You: loop\n",
            "GPT-2:  say\n",
            "You: something is wrong here!\n",
            "GPT-2: 1\n",
            "You: Awesome chat\n",
            "GPT-2: :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of repetitive behavior of Chatbot\n",
        "\n",
        "The repetitive behavior can be a bit perplexing. Here's a step-by-step breakdown of why this might happen:\n",
        "\n",
        "1. **Model's Training Data**: GPT-2 is trained on vast amounts of text from the internet. If certain patterns or phrases are more common in the training data, the model might be biased towards generating them.\n",
        "\n",
        "2. **Lack of Contextual Understanding**: The model doesn't truly \"understand\" context as humans do. When it receives input like \"hello mr\", it might latch onto that pattern and simply repeat it, thinking it's a valid response given the limited context it has.\n",
        "\n",
        "3. **Temperature Setting**: The `temperature` parameter in the `generate` function controls the randomness of the model's outputs. A higher value makes the output more random, while a lower value makes it more deterministic. If the temperature is set too low, the model might end up in a loop, generating the same or similar text repeatedly. Adjusting this parameter might help in producing varied outputs.\n",
        "\n",
        "4. **Max Length and Top-K Sampling**: Other parameters that can influence the diversity of the generated text are the `max_length` and sampling strategies like `top_k` or `top_p`. By limiting the choices the model can pick from, or by truncating its output, we might inadvertently be promoting repetitive behavior.\n",
        "\n",
        "5. **Accumulated Chat History**: In our chatbot loop, we keep appending the model's output to the chat history. If the model starts generating repetitive text, that repetitive content becomes part of the subsequent input, exacerbating the issue.\n",
        "\n",
        "6. **GPT-2's Limitations**: Even though GPT-2 is impressive, it's not perfect. It can generate coherent passages of text, but it can also generate gibberish or fall into repetitive patterns. The behavior is partly a reflection of its training data and the inherent limitations of the model.\n",
        "\n",
        "To improve the chatbot:\n",
        "\n",
        "- **Temperature Adjustment**: Try adjusting the `temperature` parameter to introduce more randomness.\n",
        "- **Introduce Breaks**: If the model starts repeating itself, you can introduce logic to break out of the loop or change its response.\n",
        "- **Limit Chat History**: Instead of always appending to the chat history, consider using only the most recent messages to prevent old, repetitive outputs from influencing new ones.\n",
        "\n",
        "Remember, while GPT-2 is a leap forward from its predecessor, it's still not perfect. It's a tool that can generate human-like text but doesn't truly understand the content it's producing."
      ],
      "metadata": {
        "id": "yKaRZ2uAOI30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-3"
      ],
      "metadata": {
        "id": "Q5S6cg49OoNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI provides access to GPT-3 through their API\n",
        "\n",
        "As of my last update in September 2021, OpenAI has not released the weights for GPT-3 on the Hugging Face model hub. This means that while there are tokenizers and configuration files for GPT-3 on Hugging Face (to allow users to use their own trained versions), the pre-trained models from OpenAI are not available there.\n",
        "\n",
        "However, OpenAI provides access to GPT-3 through their API, which allows developers to integrate GPT-3 into applications, products, or services.\n",
        "\n",
        "It's always a good idea to check the official Hugging Face model hub or OpenAI's official resources to see if there have been any updates regarding GPT-3's availability."
      ],
      "metadata": {
        "id": "lNNQUCoeOp-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary"
      ],
      "metadata": {
        "id": "kXj_A8M2oGMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.BCELoss`\n",
        "\n",
        "### Step 1: Understanding BCELoss\n",
        "\n",
        "`nn.BCELoss` stands for Binary Cross-Entropy Loss. It's a loss function commonly used in binary classification tasks, where the objective is to classify instances into one of two classes.\n",
        "\n",
        "Given:\n",
        "- \\( p \\): The predicted probability of the instance belonging to class 1.\n",
        "- \\( y \\): The true label of the instance (either 0 or 1).\n",
        "\n",
        "The formula for binary cross-entropy for a single instance is:\n",
        "\n",
        "$$\n",
        "-\\left( y \\cdot \\log(p) + (1-y) \\cdot \\log(1-p) \\right)\n",
        "$$\n",
        "\n",
        "If \\( y = 1 \\) (i.e., the true label is class 1), the loss becomes \\( -\\log(p) \\), meaning the closer \\( p \\) is to 1, the lower the loss. Conversely, if \\( y = 0 \\), the loss is \\( -\\log(1-p) \\), implying the closer \\( p \\) is to 0, the lower the loss.\n",
        "\n",
        "### Step 2: An Analogy\n",
        "\n",
        "Imagine you're teaching a child the difference between hot and cold temperatures using a thermometer. Every time they make a guess, you correct them, and the further their guess is from the actual temperature, the more you emphasize their mistake.\n",
        "\n",
        "- If the actual temperature is hot (let's say this is equivalent to class 1), and they guess it's very cold (a probability close to 0), you'd correct them quite firmly because they're way off.\n",
        "  \n",
        "- Similarly, if the actual temperature is cold (equivalent to class 0), and they guess it's boiling hot (a probability close to 1), you'd again correct them with emphasis.\n",
        "\n",
        "- But, if their guesses are close to the actual temperature, your corrections would be gentle.\n",
        "\n",
        "In this analogy:\n",
        "- The child's guess is the predicted probability \\( p \\).\n",
        "- The actual temperature is the true label \\( y \\).\n",
        "- The firmness or gentleness of your correction is the BCELoss. The further the guess is from the truth, the higher the loss.\n",
        "\n",
        "### Step 3: Why Use BCELoss?\n",
        "\n",
        "- It's continuous: Even a small change in prediction can lead to a change in loss, which helps models make incremental improvements.\n",
        "  \n",
        "- It penalizes confident wrong predictions: If the model predicts a high probability for the incorrect class, the loss will be high.\n",
        "  \n",
        "- It's differentiable: This is important for optimization algorithms (like gradient descent) to work.\n",
        "\n",
        "In conclusion, `nn.BCELoss` is a measure of how far off our model's predictions are from the truth in binary classification tasks. It penalizes wrong predictions, especially those made with high confidence, guiding the model to improve its predictions over time."
      ],
      "metadata": {
        "id": "6WUVZQv8oIE-"
      }
    }
  ]
}